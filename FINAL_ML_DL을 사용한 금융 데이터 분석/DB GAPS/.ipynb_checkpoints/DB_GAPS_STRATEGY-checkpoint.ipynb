{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ba4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RSI \n",
    "# 가격이 전일 가격보다 상승한 날의 상승분은 U(up) 값이라고 하고,\n",
    "# 가격이 전일 가격보다 하락한 날의 하락분은 D(down) 값이라고 한다.\n",
    "# U값과 D값의 평균값을 구하여 그것을 각각 AU(average ups)와 AD(average downs)라 한다.\n",
    "# AU를 AD값으로 나눈 것을 RS(relative strength) 값이라고 한다. RS 값이 크다는 것은 일정 기간 하락한 폭보다 상승한 폭이 크다는 것을 의미한다.\n",
    "# 다음 계산에 의하여 RSI 값을 구한다.\n",
    "\n",
    "# RSI 계산 공식 :\n",
    "\n",
    "# RSI = RS / (1 + RS)\n",
    "\n",
    "# 또는, 다음과 같이 구해도 결과는 동일하다.\n",
    "\n",
    "# RSI = AU / (AU + AD)\n",
    "\n",
    "# 대체로 이 값은 백분율로 나타낸다.\n",
    "\n",
    "# 이 지표의 파라메터로는 기간을 며칠 동안으로 할 것인가가 있다. Welles Wilder는 14일을 사용할 것을 권유했다. 대체로 사용되는 값은 9일, 14~15일, 25~28일 등이다.\n",
    "\n",
    "# RSI 그래프는 이동평균선을 함께 나타내는 것이 보통이며, 이동평균선을 며칠선으로 할 것인가 역시 파라메터로 주어진다. RSI를 15일에 대하여 구하고 5일 이동평균선을 함께 표시하는 경우 그래프에 (15, 5)라고 표시해주는 것이 일반적이다.\n",
    "\n",
    "# 유사한 지표로는 스토캐스틱이 있다. RSI 그래프의 형태는 fast stochastic과 비슷하게 나온다.\n",
    "\n",
    "# 스토캐스틱 %K = (현재가격 - N일중 최저가)/(N일중 최고가 - N일중 최저가) * 100\n",
    "# 스토캐스틱 %D = m일 동안 %K 평균 = Slow %K\n",
    "# 이는 패스트 스토캐스틱 : 슬로우 캐스틱도 고려해볼것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e5c39",
   "metadata": {},
   "source": [
    "> 기본적 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec6ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import ensemble\n",
    "from vecstack import StackingTransformer\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from vecstack import stacking\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from vecstack import stacking\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08f3f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_dict = {'KODEX_200' : '069500', '코스닥150' : '232080', 'S&P500' : '143850', 'Euro_stoxx' : '195930', 'Nikkei225' : '238720', 'CSI300' : '192090', \n",
    "           'Gold' : '132030', 'WTI' : '130680', 'KODEX_Inverse' : '114800', 'Dollar' : '138230', 'Dollar_Inverse' : '139660'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24a9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RSI_14(data):\n",
    "    RSI_list = []\n",
    "    for i in range(15, len(data)):  # 15행 종가부터 시작\n",
    "        close = list(data.iloc[i - 14 : i + 1]['Close']) # [23665, 23572, 23676, ...]\n",
    "        positive = []\n",
    "        negative = []\n",
    "        for j in range(14):\n",
    "            diff = close[j + 1] - close[j]\n",
    "            if diff >= 0:\n",
    "                positive.append(diff)\n",
    "            else:\n",
    "                negative.append(diff)\n",
    "        \n",
    "        AU = np.sum(positive) / 13\n",
    "        AD = abs(np.sum(negative) / 13)\n",
    "        RSI = AU / (AU + AD)\n",
    "        RSI_list.append(RSI)\n",
    "        \n",
    "    while len(RSI_list) != len(data):\n",
    "        RSI_list.insert(0, 0)\n",
    "\n",
    "    return RSI_list\n",
    "\n",
    "def stocastic_k(data):   # 14 days\n",
    "    null_list = []\n",
    "    for i in range(len(data)):\n",
    "        calculate_low = np.array(data['Low'][i - 13 : i + 1])\n",
    "        calculate_high = np.array(data['High'][i - 13 : i + 1])\n",
    "        if str(calculate_low.mean()) == 'nan':\n",
    "            continue\n",
    "        else:\n",
    "            today = data.iloc[i]['Close']\n",
    "            mini = calculate_low.min()\n",
    "            high = calculate_high.max()\n",
    "            null_list.append((today - mini) / (high - mini))\n",
    "        \n",
    "    while len(null_list) != len(data):\n",
    "        null_list.insert(0, 0)\n",
    "    \n",
    "    return null_list\n",
    "\n",
    "\n",
    "def Bollinger(data):    # 20 , 2\n",
    "    null_list = []\n",
    "    for i in range(len(data)):\n",
    "        cal_list = data['Close'][i - 19 : i + 1]\n",
    "        high = cal_list.mean() + np.std(cal_list) * 2\n",
    "        low = cal_list.mean() - np.std(cal_list) * 2\n",
    "        position = (data['Close'][i] - low) /  (high - low)\n",
    "        null_list.append(position)\n",
    "    return null_list\n",
    "\n",
    "def last_day(code, year):\n",
    "    day = str(fdr.DataReader(str(code), str(year), str(year + 1)).index[-1])[:10]\n",
    "    print(day)\n",
    "    \n",
    "    return day\n",
    "\n",
    "def start_day(code, year):\n",
    "    day = str(fdr.DataReader(str(code), str(year), str(year + 1)).index[0])[:10]\n",
    "    print(day)\n",
    "    \n",
    "    return day\n",
    "\n",
    "def last_day_month(code, year, month):\n",
    "    day = str(fdr.DataReader(str(code), str(year) + '.' + str(month), str(year) + '.' + str(month + 1)).index[-1])[:10]\n",
    "    \n",
    "    return day\n",
    "def start_day_month(code, year, month):\n",
    "    day = str(fdr.DataReader(str(code), str(year) + '.' + str(month), str(year) + '.' + str(month + 1)).index[0])[:10]    \n",
    "    \n",
    "    return day\n",
    "\n",
    "def MACD_cat(data):\n",
    "    null_list = []\n",
    "    for i in range(1, len(data)):\n",
    "        if (data['MACD'][i] > 0) & (data['MACD'][i - 1] < 0):\n",
    "            null_list.append(1)\n",
    "        else:\n",
    "            null_list.append(0)\n",
    "    null_list.insert(0, 0)\n",
    "    return null_list\n",
    "\n",
    "# def MACD_ocs(data):\n",
    "#     null_list = []\n",
    "#     null_list2 = []\n",
    "#     cont_list = data['MACD'] - data['MACD_SIGNAL']\n",
    "#     for i in range(1, cont_list):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1bd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all(data):\n",
    "    data['RSI'] = get_RSI_14(data)\n",
    "    data['STOCASTIC_K'] = stocastic_k(data)\n",
    "    data['STOCASTIC_D'] = data['STOCASTIC_K'].ewm(span = 5).mean()  # 5일\n",
    "    data['Bollinger'] = Bollinger(data)\n",
    "    data['MACD'] = data['Close'].ewm(span = 12).mean() - data['Close'].ewm(span = 26).mean()\n",
    "    data['MACD_SIGNAL'] = data['MACD'].ewm(span = 9).mean()\n",
    "    data['MACD_cat'] = MACD_cat(data)\n",
    "    data['Change+'] = list((data['Change'] > 0)[1 : len(data)].astype(int)) + [0]\n",
    "    \n",
    "    data['RSI_delta'] = data.RSI.diff().fillna(0)\n",
    "#     data['K_delta'] = data.STOCASTIC_K.diff().fillna(0)\n",
    "    data['D_delta'] = data.STOCASTIC_D.diff().fillna(0)\n",
    "    data['sto_diff'] = data['STOCASTIC_K'] - data['STOCASTIC_D']\n",
    "    data['B_delta'] = data.Bollinger.diff().fillna(0)\n",
    "    data['MACD_delta'] = data.MACD.diff().fillna(0)\n",
    "#     ma20 = new_gs['Adj Close'].rolling(window=20).mean()\n",
    "    data['MA5'] = data['Close'].rolling(window=5).mean()\n",
    "    data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['MA5_adj'] = (data['MA5'] - data['Close']) / data['Close']\n",
    "    data['MA20_adj'] = (data['MA20'] - data['Close']) / data['Close']\n",
    "    data['MA_diff'] = (data['MA5'] - data['MA20']) / data['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd533f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kospi = fdr.DataReader('069500', '2010', '2021')\n",
    "all(df_kospi)\n",
    "df_kospi.dropna(inplace = True)\n",
    "df_kospi = df_kospi[29 : ]            # 일종의 문법\n",
    "\n",
    "# df_kospi.dropna(inplace = True)\n",
    "\n",
    "# df_kospi.replace([np.inf, -np.inf], np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6272c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kospi3 = fdr.DataReader('069500', '2022', '2023')\n",
    "all(df_kospi3)\n",
    "df_kospi3 = df_kospi3.iloc[26 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "794b0553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b80930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "# result2 = pd.concat([df1,df2], ignore_index=True)\n",
    "from pykrx import stock\n",
    "stock_code = stock.get_market_ticker_list(date=\"20201020\", market=\"KOSPI\")\n",
    "\n",
    "np.random.shuffle(stock_code)\n",
    "\n",
    "company_list = stock_code[:400]\n",
    "\n",
    "j = 1\n",
    "for i in company_list:\n",
    "    print(j)\n",
    "    data = fdr.DataReader(i, '2000', '2022')\n",
    "    if len(data) < 30:\n",
    "        continue\n",
    "    else:\n",
    "        all(data)\n",
    "        data = data.dropna()\n",
    "        df_kospi = pd.concat([df_kospi, data[29:]])\n",
    "    j += 1\n",
    "\n",
    "df_kospi.dropna(inplace = True)\n",
    "\n",
    "df_kospi.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "# df_kospi.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace = True)\n",
    "df_kospi.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74dbb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kospi.to_csv('shuffle_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1d6811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.756744e+06\n",
       "mean     1.718406e-02\n",
       "std      1.299568e-01\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      0.000000e+00\n",
       "max      1.000000e+00\n",
       "Name: MACD_cat, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kospi.describe()['MACD_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5112360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kospi.describe().to_csv('cat_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "603d1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_kospi.describe()\n",
    "mean = a.iloc[:, 6:].loc['mean']\n",
    "std = a.iloc[:, 6:].loc['std']\n",
    "\n",
    "mean.to_csv('mean.csv')\n",
    "std.to_csv('std.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3c0b8",
   "metadata": {},
   "source": [
    " > 모델링 : 실험1 (기본적 전처리만, 2010~2021 -> 2022) : 57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7caf9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = df_x.loc[: last_day('069500', 2021)]\n",
    "# train_y = df_y.loc[: last_day('069500', 2021)]\n",
    "# test_x = df_x.loc[start_day('069500', 2022) :]\n",
    "# test_y = df_y.loc[start_day('069500', 2022) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80dda8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # regularization candiate 정의\n",
    "# reg_candidate = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 5, 10, 100]\n",
    "\n",
    "# # space 정의, Hyperparameter의 이름을 key 값으로 입력\n",
    "# space={'max_depth': hp.quniform(\"max_depth\", 4, 20, 2),\n",
    "#        'learning_rate': hp.quniform ('learning_rate', 0.001, 0.01, 0.001),\n",
    "#        'reg_alpha' : hp.choice('reg_alpha', reg_candidate),\n",
    "#        'reg_lambda' : hp.choice('reg_lambda', reg_candidate),\n",
    "#        'subsample': hp.quniform('subsample', 0.6, 1, 0.05),\n",
    "#        'colsample_bytree' : hp.quniform('colsample_bytree', 0.6, 1, 0.05),\n",
    "#        'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "#        'n_estimators': hp.quniform('n_estimators', 300, 3000, 300)}\n",
    "\n",
    "# # 목적 함수 정의\n",
    "# # n_estimators, max_depth와 같은 반드시 int 타입을 가져야 하는 hyperparamter는 int로 타입 캐스팅 합니다.\n",
    "# def hyperparameter_tuning(space):\n",
    "#     model=XGBClassifier(n_estimators =int(space['n_estimators']), \n",
    "#                        max_depth = int(space['max_depth']), \n",
    "#                        learning_rate = space['learning_rate'],\n",
    "#                        reg_alpha = space['reg_alpha'],\n",
    "#                        reg_lambda = space['reg_lambda'],\n",
    "#                        subsample = space['subsample'],\n",
    "#                        colsample_bytree = space['colsample_bytree'], \n",
    "#                        min_child_weight = int(space['min_child_weight']),\n",
    "#                        random_state=42,)\n",
    "    \n",
    "#     evaluation = [(train_x, train_y), (test_x, test_y)]\n",
    "    \n",
    "#     model.fit(train_x, train_y,\n",
    "#               eval_set=evaluation, \n",
    "#               eval_metric='auc',\n",
    "#               verbose=0)\n",
    "\n",
    "#     accuracy = accuracy_score(model.predict(test_x), test_y)\n",
    "#     # 평가 방식 선정\n",
    "#     return {'loss': 1 - accuracy, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# # Trials 객체 선언합니다.\n",
    "# trials = Trials()\n",
    "# # best에 최적의 하이퍼 파라미터를 return 받습니다.\n",
    "# best = fmin(fn=hyperparameter_tuning,\n",
    "#             space=space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
    "#             trials=trials)\n",
    "\n",
    "# # 최적화된 결과를 int로 변환해야하는 파라미터는 타입 변환을 수행합니다.\n",
    "# best['max_depth'] = int(best['max_depth'])\n",
    "# best['min_child_weight'] = int(best['min_child_weight'])\n",
    "# best['n_estimators'] = int(best['n_estimators'])\n",
    "# best['reg_alpha'] = reg_candidate[int(best['reg_alpha'])]\n",
    "# best['reg_lambda'] = reg_candidate[int(best['reg_lambda'])]\n",
    "# print (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3154367",
   "metadata": {},
   "source": [
    "> 실험 2 : 2022 -> 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e79380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_day_month('069500', 2022, 1)\n",
    "# last_day_month('069500', 2022, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71185922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = df_x.loc[start_day_month('069500', 2022, 1) : last_day_month('069500', 2022, 4)]\n",
    "# train_y = df_y.loc[start_day_month('069500', 2022, 1) : last_day_month('069500', 2022, 4)]\n",
    "# test_x = df_x.loc[start_day_month('069500', 2022, 5) : last_day_month('069500', 2022, 5)]\n",
    "# test_y = df_y.loc[start_day_month('069500', 2022, 5) : last_day_month('069500', 2022, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c522833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier()\n",
    "# model.fit(train_x, train_y)\n",
    "# accuracy_score(model.predict(train_x), train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16d3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cat = CatBoostClassifier()\n",
    "# model_cat.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a8c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(model.predict(test_x), test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a73c43",
   "metadata": {},
   "source": [
    "> 실험 3 : Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec1330",
   "metadata": {},
   "source": [
    "<!-- numeric_features = list(train_x.columns)\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = []\n",
    "categorical_transformer = OneHotEncoder(categories='auto')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [ ('num', numeric_transformer, numeric_features),\n",
    "        ('passthrough', 'passthrough', categorical_features)])\n",
    "\n",
    "preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "preprocessor_pipe.fit(train_x)\n",
    "\n",
    "x_train_transformed = preprocessor_pipe.transform(train_x)\n",
    "x_test_transformed = preprocessor_pipe.transform(test_x) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f307766",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b99a883946b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# list(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a2670ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_features = list(train_x.columns)\n",
    "# numeric_transformer = StandardScaler() # cf) RobustScaler\n",
    "\n",
    "# categorical_features = []\n",
    "# categorical_transformer = OneHotEncoder(categories='auto') # categories='auto' : just for ignoring warning messages\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers = [ ('num', numeric_transformer, numeric_features),\n",
    "#         ('passthrough', 'passthrough', categorical_features)])\n",
    "\n",
    "# preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# preprocessor_pipe.fit(train_x)\n",
    "\n",
    "# x_train_transformed = preprocessor_pipe.transform(train_x)\n",
    "# x_test_transformed = preprocessor_pipe.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4757c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier(learning_rate=0.001,max_depth=5,n_estimators=100)\n",
    "# model.fit(x_train_transformed, train_y)\n",
    "# accuracy_score(model.predict(x_test_transformed), test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ecd69",
   "metadata": {},
   "source": [
    "> 실험 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6fbd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = df_x.loc[: last_day('069500', 2020)]\n",
    "# train_y = df_y.loc[: last_day('069500', 2020)]\n",
    "# test_x = df_x.loc[start_day('069500', 2021) : last_day('069500', 2021)]\n",
    "# test_y = df_y.loc[start_day('069500', 2021) : last_day('069500', 2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81430b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df_kospi[['RSI', 'STOCASTIC_K', 'STOCASTIC_D', 'Bollinger', 'MACD', 'MACD_cat', 'MACD_SIGNAL', 'RSI_delta', 'D_delta', 'sto_diff', 'B_delta', 'MACD_delta',\n",
    "                   'MA5', 'MA20', 'MA5_adj', 'MA20_adj']]\n",
    "train_y = df_kospi['Change+']\n",
    "test_x = df_kospi3[['RSI', 'STOCASTIC_K', 'STOCASTIC_D', 'Bollinger', 'MACD', 'MACD_cat', 'MACD_SIGNAL', 'RSI_delta', 'D_delta', 'sto_diff', 'B_delta', 'MACD_delta',\n",
    "                   'MA5', 'MA20', 'MA5_adj', 'MA20_adj']]\n",
    "test_y = df_kospi3['Change+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e5aa322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1756744, 16)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dbf6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['RSI', 'STOCASTIC_K', 'STOCASTIC_D', 'Bollinger', 'MACD', 'MACD_SIGNAL', 'RSI_delta', 'D_delta', 'sto_diff', 'B_delta', 'MACD_delta', 'MA5', 'MA20',\n",
    "                   'MA5_adj', 'MA20_adj']\n",
    "numeric_transformer = StandardScaler() # cf) RobustScaler\n",
    "\n",
    "categorical_features = ['MACD_cat']\n",
    "categorical_transformer = OneHotEncoder(categories='auto') # categories='auto' : just for ignoring warning messages\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [ ('num', numeric_transformer, numeric_features),\n",
    "        ('passthrough', categorical_transformer, categorical_features)])\n",
    "\n",
    "preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "preprocessor_pipe.fit(train_x)\n",
    "\n",
    "train_x = preprocessor_pipe.transform(train_x)\n",
    "test_x = preprocessor_pipe.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07f90199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1756744, 17)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b32ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.250279\n",
      "0:\tlearn: 0.6899530\ttotal: 262ms\tremaining: 4m 21s\n",
      "1:\tlearn: 0.6880810\ttotal: 367ms\tremaining: 3m 2s\n",
      "2:\tlearn: 0.6870060\ttotal: 482ms\tremaining: 2m 40s\n",
      "3:\tlearn: 0.6863007\ttotal: 592ms\tremaining: 2m 27s\n",
      "4:\tlearn: 0.6858740\ttotal: 707ms\tremaining: 2m 20s\n",
      "5:\tlearn: 0.6855736\ttotal: 843ms\tremaining: 2m 19s\n",
      "6:\tlearn: 0.6853185\ttotal: 992ms\tremaining: 2m 20s\n",
      "7:\tlearn: 0.6851511\ttotal: 1.13s\tremaining: 2m 20s\n",
      "8:\tlearn: 0.6850515\ttotal: 1.24s\tremaining: 2m 16s\n",
      "9:\tlearn: 0.6849870\ttotal: 1.36s\tremaining: 2m 14s\n",
      "10:\tlearn: 0.6848741\ttotal: 1.46s\tremaining: 2m 10s\n",
      "11:\tlearn: 0.6847656\ttotal: 1.59s\tremaining: 2m 10s\n",
      "12:\tlearn: 0.6847066\ttotal: 1.7s\tremaining: 2m 9s\n",
      "13:\tlearn: 0.6846325\ttotal: 1.82s\tremaining: 2m 8s\n",
      "14:\tlearn: 0.6844683\ttotal: 1.95s\tremaining: 2m 7s\n",
      "15:\tlearn: 0.6843933\ttotal: 2.06s\tremaining: 2m 6s\n",
      "16:\tlearn: 0.6843376\ttotal: 2.19s\tremaining: 2m 6s\n",
      "17:\tlearn: 0.6842693\ttotal: 2.29s\tremaining: 2m 5s\n",
      "18:\tlearn: 0.6842399\ttotal: 2.4s\tremaining: 2m 4s\n",
      "19:\tlearn: 0.6841816\ttotal: 2.55s\tremaining: 2m 5s\n",
      "20:\tlearn: 0.6841518\ttotal: 2.67s\tremaining: 2m 4s\n",
      "21:\tlearn: 0.6840648\ttotal: 2.81s\tremaining: 2m 4s\n",
      "22:\tlearn: 0.6840099\ttotal: 2.95s\tremaining: 2m 5s\n",
      "23:\tlearn: 0.6839747\ttotal: 3.08s\tremaining: 2m 5s\n",
      "24:\tlearn: 0.6839294\ttotal: 3.2s\tremaining: 2m 4s\n",
      "25:\tlearn: 0.6839032\ttotal: 3.33s\tremaining: 2m 4s\n",
      "26:\tlearn: 0.6838604\ttotal: 3.45s\tremaining: 2m 4s\n",
      "27:\tlearn: 0.6838169\ttotal: 3.57s\tremaining: 2m 4s\n",
      "28:\tlearn: 0.6837917\ttotal: 3.66s\tremaining: 2m 2s\n",
      "29:\tlearn: 0.6837531\ttotal: 3.78s\tremaining: 2m 2s\n",
      "30:\tlearn: 0.6837160\ttotal: 3.91s\tremaining: 2m 2s\n",
      "31:\tlearn: 0.6836863\ttotal: 4.02s\tremaining: 2m 1s\n",
      "32:\tlearn: 0.6836172\ttotal: 4.17s\tremaining: 2m 2s\n",
      "33:\tlearn: 0.6835905\ttotal: 4.28s\tremaining: 2m 1s\n",
      "34:\tlearn: 0.6835576\ttotal: 4.41s\tremaining: 2m 1s\n",
      "35:\tlearn: 0.6835281\ttotal: 4.53s\tremaining: 2m 1s\n",
      "36:\tlearn: 0.6834996\ttotal: 4.64s\tremaining: 2m\n",
      "37:\tlearn: 0.6834773\ttotal: 4.76s\tremaining: 2m\n",
      "38:\tlearn: 0.6834558\ttotal: 4.88s\tremaining: 2m\n",
      "39:\tlearn: 0.6834315\ttotal: 5s\tremaining: 1m 59s\n",
      "40:\tlearn: 0.6834039\ttotal: 5.13s\tremaining: 1m 59s\n",
      "41:\tlearn: 0.6833820\ttotal: 5.25s\tremaining: 1m 59s\n",
      "42:\tlearn: 0.6833531\ttotal: 5.38s\tremaining: 1m 59s\n",
      "43:\tlearn: 0.6833250\ttotal: 5.52s\tremaining: 1m 59s\n",
      "44:\tlearn: 0.6832979\ttotal: 5.63s\tremaining: 1m 59s\n",
      "45:\tlearn: 0.6832686\ttotal: 5.76s\tremaining: 1m 59s\n",
      "46:\tlearn: 0.6832470\ttotal: 5.87s\tremaining: 1m 59s\n",
      "47:\tlearn: 0.6832239\ttotal: 6s\tremaining: 1m 58s\n",
      "48:\tlearn: 0.6832006\ttotal: 6.12s\tremaining: 1m 58s\n",
      "49:\tlearn: 0.6831769\ttotal: 6.25s\tremaining: 1m 58s\n",
      "50:\tlearn: 0.6831510\ttotal: 6.37s\tremaining: 1m 58s\n",
      "51:\tlearn: 0.6831231\ttotal: 6.48s\tremaining: 1m 58s\n",
      "52:\tlearn: 0.6831034\ttotal: 6.57s\tremaining: 1m 57s\n",
      "53:\tlearn: 0.6830881\ttotal: 6.7s\tremaining: 1m 57s\n",
      "54:\tlearn: 0.6830732\ttotal: 6.81s\tremaining: 1m 57s\n",
      "55:\tlearn: 0.6830417\ttotal: 6.93s\tremaining: 1m 56s\n",
      "56:\tlearn: 0.6830145\ttotal: 7.04s\tremaining: 1m 56s\n",
      "57:\tlearn: 0.6829853\ttotal: 7.18s\tremaining: 1m 56s\n",
      "58:\tlearn: 0.6829631\ttotal: 7.3s\tremaining: 1m 56s\n",
      "59:\tlearn: 0.6829395\ttotal: 7.4s\tremaining: 1m 55s\n",
      "60:\tlearn: 0.6829153\ttotal: 7.52s\tremaining: 1m 55s\n",
      "61:\tlearn: 0.6828931\ttotal: 7.62s\tremaining: 1m 55s\n",
      "62:\tlearn: 0.6828407\ttotal: 7.76s\tremaining: 1m 55s\n",
      "63:\tlearn: 0.6828192\ttotal: 7.86s\tremaining: 1m 54s\n",
      "64:\tlearn: 0.6827935\ttotal: 7.98s\tremaining: 1m 54s\n",
      "65:\tlearn: 0.6827710\ttotal: 8.1s\tremaining: 1m 54s\n",
      "66:\tlearn: 0.6827514\ttotal: 8.21s\tremaining: 1m 54s\n",
      "67:\tlearn: 0.6827271\ttotal: 8.33s\tremaining: 1m 54s\n",
      "68:\tlearn: 0.6827012\ttotal: 8.46s\tremaining: 1m 54s\n",
      "69:\tlearn: 0.6826806\ttotal: 8.58s\tremaining: 1m 53s\n",
      "70:\tlearn: 0.6826582\ttotal: 8.7s\tremaining: 1m 53s\n",
      "71:\tlearn: 0.6826273\ttotal: 8.82s\tremaining: 1m 53s\n",
      "72:\tlearn: 0.6826019\ttotal: 8.94s\tremaining: 1m 53s\n",
      "73:\tlearn: 0.6825789\ttotal: 9.06s\tremaining: 1m 53s\n",
      "74:\tlearn: 0.6825595\ttotal: 9.16s\tremaining: 1m 53s\n",
      "75:\tlearn: 0.6825387\ttotal: 9.28s\tremaining: 1m 52s\n",
      "76:\tlearn: 0.6825183\ttotal: 9.4s\tremaining: 1m 52s\n",
      "77:\tlearn: 0.6825034\ttotal: 9.52s\tremaining: 1m 52s\n",
      "78:\tlearn: 0.6824782\ttotal: 9.63s\tremaining: 1m 52s\n",
      "79:\tlearn: 0.6824525\ttotal: 9.76s\tremaining: 1m 52s\n",
      "80:\tlearn: 0.6824329\ttotal: 9.88s\tremaining: 1m 52s\n",
      "81:\tlearn: 0.6824179\ttotal: 10s\tremaining: 1m 52s\n",
      "82:\tlearn: 0.6823970\ttotal: 10.1s\tremaining: 1m 51s\n",
      "83:\tlearn: 0.6823862\ttotal: 10.3s\tremaining: 1m 51s\n",
      "84:\tlearn: 0.6823680\ttotal: 10.4s\tremaining: 1m 51s\n",
      "85:\tlearn: 0.6823498\ttotal: 10.5s\tremaining: 1m 51s\n",
      "86:\tlearn: 0.6823225\ttotal: 10.6s\tremaining: 1m 51s\n",
      "87:\tlearn: 0.6823070\ttotal: 10.7s\tremaining: 1m 51s\n",
      "88:\tlearn: 0.6822923\ttotal: 10.9s\tremaining: 1m 51s\n",
      "89:\tlearn: 0.6822714\ttotal: 11s\tremaining: 1m 51s\n",
      "90:\tlearn: 0.6822519\ttotal: 11.1s\tremaining: 1m 50s\n",
      "91:\tlearn: 0.6822388\ttotal: 11.2s\tremaining: 1m 50s\n",
      "92:\tlearn: 0.6822229\ttotal: 11.3s\tremaining: 1m 50s\n",
      "93:\tlearn: 0.6822086\ttotal: 11.5s\tremaining: 1m 50s\n",
      "94:\tlearn: 0.6821922\ttotal: 11.6s\tremaining: 1m 50s\n",
      "95:\tlearn: 0.6821692\ttotal: 11.8s\tremaining: 1m 50s\n",
      "96:\tlearn: 0.6821454\ttotal: 11.9s\tremaining: 1m 50s\n",
      "97:\tlearn: 0.6821276\ttotal: 12.1s\tremaining: 1m 50s\n",
      "98:\tlearn: 0.6821039\ttotal: 12.2s\tremaining: 1m 50s\n",
      "99:\tlearn: 0.6820880\ttotal: 12.3s\tremaining: 1m 50s\n",
      "100:\tlearn: 0.6820684\ttotal: 12.5s\tremaining: 1m 50s\n",
      "101:\tlearn: 0.6820544\ttotal: 12.6s\tremaining: 1m 51s\n",
      "102:\tlearn: 0.6820357\ttotal: 12.7s\tremaining: 1m 50s\n",
      "103:\tlearn: 0.6820179\ttotal: 12.9s\tremaining: 1m 50s\n",
      "104:\tlearn: 0.6820050\ttotal: 13s\tremaining: 1m 51s\n",
      "105:\tlearn: 0.6819707\ttotal: 13.2s\tremaining: 1m 51s\n",
      "106:\tlearn: 0.6819537\ttotal: 13.3s\tremaining: 1m 51s\n",
      "107:\tlearn: 0.6819384\ttotal: 13.5s\tremaining: 1m 51s\n",
      "108:\tlearn: 0.6819158\ttotal: 13.6s\tremaining: 1m 51s\n",
      "109:\tlearn: 0.6819014\ttotal: 13.7s\tremaining: 1m 51s\n",
      "110:\tlearn: 0.6818836\ttotal: 13.9s\tremaining: 1m 51s\n",
      "111:\tlearn: 0.6818697\ttotal: 14s\tremaining: 1m 51s\n",
      "112:\tlearn: 0.6818441\ttotal: 14.2s\tremaining: 1m 51s\n",
      "113:\tlearn: 0.6818267\ttotal: 14.3s\tremaining: 1m 51s\n",
      "114:\tlearn: 0.6818130\ttotal: 14.5s\tremaining: 1m 51s\n",
      "115:\tlearn: 0.6817940\ttotal: 14.6s\tremaining: 1m 51s\n",
      "116:\tlearn: 0.6817779\ttotal: 14.7s\tremaining: 1m 51s\n",
      "117:\tlearn: 0.6817617\ttotal: 14.9s\tremaining: 1m 51s\n",
      "118:\tlearn: 0.6817454\ttotal: 15s\tremaining: 1m 51s\n",
      "119:\tlearn: 0.6817311\ttotal: 15.1s\tremaining: 1m 50s\n",
      "120:\tlearn: 0.6817149\ttotal: 15.3s\tremaining: 1m 50s\n",
      "121:\tlearn: 0.6816968\ttotal: 15.4s\tremaining: 1m 51s\n",
      "122:\tlearn: 0.6816831\ttotal: 15.6s\tremaining: 1m 51s\n",
      "123:\tlearn: 0.6816701\ttotal: 15.7s\tremaining: 1m 51s\n",
      "124:\tlearn: 0.6816558\ttotal: 15.9s\tremaining: 1m 51s\n",
      "125:\tlearn: 0.6816404\ttotal: 16s\tremaining: 1m 50s\n",
      "126:\tlearn: 0.6816205\ttotal: 16.1s\tremaining: 1m 50s\n",
      "127:\tlearn: 0.6816035\ttotal: 16.3s\tremaining: 1m 50s\n",
      "128:\tlearn: 0.6815834\ttotal: 16.4s\tremaining: 1m 50s\n",
      "129:\tlearn: 0.6815662\ttotal: 16.5s\tremaining: 1m 50s\n",
      "130:\tlearn: 0.6815528\ttotal: 16.7s\tremaining: 1m 50s\n",
      "131:\tlearn: 0.6815373\ttotal: 16.9s\tremaining: 1m 50s\n",
      "132:\tlearn: 0.6815244\ttotal: 17s\tremaining: 1m 50s\n",
      "133:\tlearn: 0.6815097\ttotal: 17.1s\tremaining: 1m 50s\n",
      "134:\tlearn: 0.6814964\ttotal: 17.2s\tremaining: 1m 50s\n",
      "135:\tlearn: 0.6814811\ttotal: 17.4s\tremaining: 1m 50s\n",
      "136:\tlearn: 0.6814662\ttotal: 17.5s\tremaining: 1m 50s\n",
      "137:\tlearn: 0.6814473\ttotal: 17.6s\tremaining: 1m 50s\n",
      "138:\tlearn: 0.6814316\ttotal: 17.8s\tremaining: 1m 50s\n",
      "139:\tlearn: 0.6814197\ttotal: 17.9s\tremaining: 1m 50s\n",
      "140:\tlearn: 0.6814028\ttotal: 18.1s\tremaining: 1m 50s\n",
      "141:\tlearn: 0.6813906\ttotal: 18.2s\tremaining: 1m 50s\n",
      "142:\tlearn: 0.6813769\ttotal: 18.4s\tremaining: 1m 50s\n",
      "143:\tlearn: 0.6813640\ttotal: 18.5s\tremaining: 1m 49s\n",
      "144:\tlearn: 0.6813492\ttotal: 18.6s\tremaining: 1m 49s\n",
      "145:\tlearn: 0.6813313\ttotal: 18.8s\tremaining: 1m 49s\n",
      "146:\tlearn: 0.6813177\ttotal: 18.9s\tremaining: 1m 49s\n",
      "147:\tlearn: 0.6813028\ttotal: 19s\tremaining: 1m 49s\n",
      "148:\tlearn: 0.6812880\ttotal: 19.2s\tremaining: 1m 49s\n",
      "149:\tlearn: 0.6812665\ttotal: 19.3s\tremaining: 1m 49s\n",
      "150:\tlearn: 0.6812546\ttotal: 19.5s\tremaining: 1m 49s\n",
      "151:\tlearn: 0.6812372\ttotal: 19.6s\tremaining: 1m 49s\n",
      "152:\tlearn: 0.6812198\ttotal: 19.8s\tremaining: 1m 49s\n",
      "153:\tlearn: 0.6812092\ttotal: 19.9s\tremaining: 1m 49s\n",
      "154:\tlearn: 0.6811891\ttotal: 20.1s\tremaining: 1m 49s\n",
      "155:\tlearn: 0.6811757\ttotal: 20.2s\tremaining: 1m 49s\n",
      "156:\tlearn: 0.6811587\ttotal: 20.3s\tremaining: 1m 49s\n",
      "157:\tlearn: 0.6811434\ttotal: 20.4s\tremaining: 1m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158:\tlearn: 0.6811302\ttotal: 20.6s\tremaining: 1m 48s\n",
      "159:\tlearn: 0.6811186\ttotal: 20.7s\tremaining: 1m 48s\n",
      "160:\tlearn: 0.6811040\ttotal: 20.8s\tremaining: 1m 48s\n",
      "161:\tlearn: 0.6810926\ttotal: 21s\tremaining: 1m 48s\n",
      "162:\tlearn: 0.6810797\ttotal: 21.1s\tremaining: 1m 48s\n",
      "163:\tlearn: 0.6810653\ttotal: 21.2s\tremaining: 1m 48s\n",
      "164:\tlearn: 0.6810529\ttotal: 21.4s\tremaining: 1m 48s\n",
      "165:\tlearn: 0.6810363\ttotal: 21.5s\tremaining: 1m 48s\n",
      "166:\tlearn: 0.6810204\ttotal: 21.7s\tremaining: 1m 48s\n",
      "167:\tlearn: 0.6810076\ttotal: 21.8s\tremaining: 1m 47s\n",
      "168:\tlearn: 0.6809949\ttotal: 21.9s\tremaining: 1m 47s\n",
      "169:\tlearn: 0.6809830\ttotal: 22.1s\tremaining: 1m 47s\n",
      "170:\tlearn: 0.6809727\ttotal: 22.2s\tremaining: 1m 47s\n",
      "171:\tlearn: 0.6809602\ttotal: 22.3s\tremaining: 1m 47s\n",
      "172:\tlearn: 0.6809412\ttotal: 22.5s\tremaining: 1m 47s\n",
      "173:\tlearn: 0.6809328\ttotal: 22.6s\tremaining: 1m 47s\n",
      "174:\tlearn: 0.6809238\ttotal: 22.8s\tremaining: 1m 47s\n",
      "175:\tlearn: 0.6809061\ttotal: 22.9s\tremaining: 1m 47s\n",
      "176:\tlearn: 0.6808927\ttotal: 23s\tremaining: 1m 46s\n",
      "177:\tlearn: 0.6808790\ttotal: 23.1s\tremaining: 1m 46s\n",
      "178:\tlearn: 0.6808672\ttotal: 23.3s\tremaining: 1m 46s\n",
      "179:\tlearn: 0.6808586\ttotal: 23.4s\tremaining: 1m 46s\n",
      "180:\tlearn: 0.6808464\ttotal: 23.5s\tremaining: 1m 46s\n",
      "181:\tlearn: 0.6808315\ttotal: 23.6s\tremaining: 1m 46s\n",
      "182:\tlearn: 0.6808183\ttotal: 23.8s\tremaining: 1m 46s\n",
      "183:\tlearn: 0.6808047\ttotal: 23.9s\tremaining: 1m 46s\n",
      "184:\tlearn: 0.6807942\ttotal: 24s\tremaining: 1m 45s\n",
      "185:\tlearn: 0.6807868\ttotal: 24.1s\tremaining: 1m 45s\n",
      "186:\tlearn: 0.6807746\ttotal: 24.3s\tremaining: 1m 45s\n",
      "187:\tlearn: 0.6807587\ttotal: 24.4s\tremaining: 1m 45s\n",
      "188:\tlearn: 0.6807436\ttotal: 24.5s\tremaining: 1m 45s\n",
      "189:\tlearn: 0.6807305\ttotal: 24.6s\tremaining: 1m 44s\n",
      "190:\tlearn: 0.6807177\ttotal: 24.8s\tremaining: 1m 44s\n",
      "191:\tlearn: 0.6807069\ttotal: 24.9s\tremaining: 1m 44s\n",
      "192:\tlearn: 0.6806918\ttotal: 25s\tremaining: 1m 44s\n",
      "193:\tlearn: 0.6806774\ttotal: 25.1s\tremaining: 1m 44s\n",
      "194:\tlearn: 0.6806648\ttotal: 25.3s\tremaining: 1m 44s\n",
      "195:\tlearn: 0.6806519\ttotal: 25.4s\tremaining: 1m 44s\n",
      "196:\tlearn: 0.6806371\ttotal: 25.6s\tremaining: 1m 44s\n",
      "197:\tlearn: 0.6806204\ttotal: 25.7s\tremaining: 1m 44s\n",
      "198:\tlearn: 0.6806047\ttotal: 25.9s\tremaining: 1m 44s\n",
      "199:\tlearn: 0.6805941\ttotal: 26s\tremaining: 1m 43s\n",
      "200:\tlearn: 0.6805699\ttotal: 26.1s\tremaining: 1m 43s\n",
      "201:\tlearn: 0.6805553\ttotal: 26.3s\tremaining: 1m 43s\n",
      "202:\tlearn: 0.6805447\ttotal: 26.4s\tremaining: 1m 43s\n",
      "203:\tlearn: 0.6805324\ttotal: 26.6s\tremaining: 1m 43s\n",
      "204:\tlearn: 0.6805197\ttotal: 26.7s\tremaining: 1m 43s\n",
      "205:\tlearn: 0.6805082\ttotal: 26.9s\tremaining: 1m 43s\n",
      "206:\tlearn: 0.6804994\ttotal: 27s\tremaining: 1m 43s\n",
      "207:\tlearn: 0.6804889\ttotal: 27.1s\tremaining: 1m 43s\n",
      "208:\tlearn: 0.6804770\ttotal: 27.3s\tremaining: 1m 43s\n",
      "209:\tlearn: 0.6804654\ttotal: 27.4s\tremaining: 1m 43s\n",
      "210:\tlearn: 0.6804517\ttotal: 27.6s\tremaining: 1m 43s\n",
      "211:\tlearn: 0.6804386\ttotal: 27.7s\tremaining: 1m 42s\n",
      "212:\tlearn: 0.6804233\ttotal: 27.9s\tremaining: 1m 42s\n",
      "213:\tlearn: 0.6804123\ttotal: 28s\tremaining: 1m 42s\n",
      "214:\tlearn: 0.6803976\ttotal: 28.1s\tremaining: 1m 42s\n",
      "215:\tlearn: 0.6803801\ttotal: 28.3s\tremaining: 1m 42s\n",
      "216:\tlearn: 0.6803691\ttotal: 28.4s\tremaining: 1m 42s\n",
      "217:\tlearn: 0.6803579\ttotal: 28.5s\tremaining: 1m 42s\n",
      "218:\tlearn: 0.6803459\ttotal: 28.7s\tremaining: 1m 42s\n",
      "219:\tlearn: 0.6803344\ttotal: 28.8s\tremaining: 1m 42s\n",
      "220:\tlearn: 0.6803154\ttotal: 29s\tremaining: 1m 42s\n",
      "221:\tlearn: 0.6803038\ttotal: 29.1s\tremaining: 1m 41s\n",
      "222:\tlearn: 0.6802886\ttotal: 29.2s\tremaining: 1m 41s\n",
      "223:\tlearn: 0.6802755\ttotal: 29.4s\tremaining: 1m 41s\n",
      "224:\tlearn: 0.6802640\ttotal: 29.5s\tremaining: 1m 41s\n",
      "225:\tlearn: 0.6802497\ttotal: 29.6s\tremaining: 1m 41s\n",
      "226:\tlearn: 0.6802376\ttotal: 29.8s\tremaining: 1m 41s\n",
      "227:\tlearn: 0.6802271\ttotal: 29.9s\tremaining: 1m 41s\n",
      "228:\tlearn: 0.6802114\ttotal: 30.1s\tremaining: 1m 41s\n",
      "229:\tlearn: 0.6802015\ttotal: 30.2s\tremaining: 1m 41s\n",
      "230:\tlearn: 0.6801880\ttotal: 30.3s\tremaining: 1m 40s\n",
      "231:\tlearn: 0.6801728\ttotal: 30.5s\tremaining: 1m 40s\n",
      "232:\tlearn: 0.6801619\ttotal: 30.6s\tremaining: 1m 40s\n",
      "233:\tlearn: 0.6801492\ttotal: 30.7s\tremaining: 1m 40s\n",
      "234:\tlearn: 0.6801358\ttotal: 30.9s\tremaining: 1m 40s\n",
      "235:\tlearn: 0.6801209\ttotal: 31s\tremaining: 1m 40s\n",
      "236:\tlearn: 0.6801124\ttotal: 31.1s\tremaining: 1m 40s\n",
      "237:\tlearn: 0.6800972\ttotal: 31.3s\tremaining: 1m 40s\n",
      "238:\tlearn: 0.6800832\ttotal: 31.4s\tremaining: 1m 40s\n",
      "239:\tlearn: 0.6800680\ttotal: 31.6s\tremaining: 1m 39s\n",
      "240:\tlearn: 0.6800536\ttotal: 31.7s\tremaining: 1m 39s\n",
      "241:\tlearn: 0.6800439\ttotal: 31.8s\tremaining: 1m 39s\n",
      "242:\tlearn: 0.6800333\ttotal: 31.9s\tremaining: 1m 39s\n",
      "243:\tlearn: 0.6800201\ttotal: 32.1s\tremaining: 1m 39s\n",
      "244:\tlearn: 0.6800115\ttotal: 32.2s\tremaining: 1m 39s\n",
      "245:\tlearn: 0.6799985\ttotal: 32.3s\tremaining: 1m 39s\n",
      "246:\tlearn: 0.6799876\ttotal: 32.5s\tremaining: 1m 39s\n",
      "247:\tlearn: 0.6799708\ttotal: 32.6s\tremaining: 1m 38s\n",
      "248:\tlearn: 0.6799586\ttotal: 32.7s\tremaining: 1m 38s\n",
      "249:\tlearn: 0.6799494\ttotal: 32.9s\tremaining: 1m 38s\n",
      "250:\tlearn: 0.6799389\ttotal: 33s\tremaining: 1m 38s\n",
      "251:\tlearn: 0.6799294\ttotal: 33.1s\tremaining: 1m 38s\n",
      "252:\tlearn: 0.6799169\ttotal: 33.2s\tremaining: 1m 38s\n",
      "253:\tlearn: 0.6799034\ttotal: 33.4s\tremaining: 1m 38s\n",
      "254:\tlearn: 0.6798931\ttotal: 33.5s\tremaining: 1m 37s\n",
      "255:\tlearn: 0.6798785\ttotal: 33.7s\tremaining: 1m 37s\n",
      "256:\tlearn: 0.6798663\ttotal: 33.8s\tremaining: 1m 37s\n",
      "257:\tlearn: 0.6798543\ttotal: 33.9s\tremaining: 1m 37s\n",
      "258:\tlearn: 0.6798441\ttotal: 34.1s\tremaining: 1m 37s\n",
      "259:\tlearn: 0.6798322\ttotal: 34.2s\tremaining: 1m 37s\n",
      "260:\tlearn: 0.6798218\ttotal: 34.4s\tremaining: 1m 37s\n",
      "261:\tlearn: 0.6798080\ttotal: 34.5s\tremaining: 1m 37s\n",
      "262:\tlearn: 0.6797958\ttotal: 34.7s\tremaining: 1m 37s\n",
      "263:\tlearn: 0.6797862\ttotal: 34.8s\tremaining: 1m 37s\n",
      "264:\tlearn: 0.6797782\ttotal: 34.9s\tremaining: 1m 36s\n",
      "265:\tlearn: 0.6797662\ttotal: 35.1s\tremaining: 1m 36s\n",
      "266:\tlearn: 0.6797565\ttotal: 35.2s\tremaining: 1m 36s\n",
      "267:\tlearn: 0.6797467\ttotal: 35.4s\tremaining: 1m 36s\n",
      "268:\tlearn: 0.6797339\ttotal: 35.5s\tremaining: 1m 36s\n",
      "269:\tlearn: 0.6797166\ttotal: 35.6s\tremaining: 1m 36s\n",
      "270:\tlearn: 0.6797038\ttotal: 35.8s\tremaining: 1m 36s\n",
      "271:\tlearn: 0.6796980\ttotal: 35.9s\tremaining: 1m 36s\n",
      "272:\tlearn: 0.6796860\ttotal: 36s\tremaining: 1m 36s\n",
      "273:\tlearn: 0.6796725\ttotal: 36.2s\tremaining: 1m 35s\n",
      "274:\tlearn: 0.6796599\ttotal: 36.3s\tremaining: 1m 35s\n",
      "275:\tlearn: 0.6796479\ttotal: 36.4s\tremaining: 1m 35s\n",
      "276:\tlearn: 0.6796342\ttotal: 36.6s\tremaining: 1m 35s\n",
      "277:\tlearn: 0.6796194\ttotal: 36.7s\tremaining: 1m 35s\n",
      "278:\tlearn: 0.6796092\ttotal: 36.8s\tremaining: 1m 35s\n",
      "279:\tlearn: 0.6796005\ttotal: 37s\tremaining: 1m 35s\n",
      "280:\tlearn: 0.6795844\ttotal: 37.1s\tremaining: 1m 34s\n",
      "281:\tlearn: 0.6795715\ttotal: 37.2s\tremaining: 1m 34s\n",
      "282:\tlearn: 0.6795587\ttotal: 37.3s\tremaining: 1m 34s\n",
      "283:\tlearn: 0.6795466\ttotal: 37.5s\tremaining: 1m 34s\n",
      "284:\tlearn: 0.6795357\ttotal: 37.6s\tremaining: 1m 34s\n",
      "285:\tlearn: 0.6795279\ttotal: 37.7s\tremaining: 1m 34s\n",
      "286:\tlearn: 0.6795169\ttotal: 37.9s\tremaining: 1m 34s\n",
      "287:\tlearn: 0.6795075\ttotal: 38s\tremaining: 1m 33s\n",
      "288:\tlearn: 0.6794938\ttotal: 38.1s\tremaining: 1m 33s\n",
      "289:\tlearn: 0.6794816\ttotal: 38.3s\tremaining: 1m 33s\n",
      "290:\tlearn: 0.6794714\ttotal: 38.4s\tremaining: 1m 33s\n",
      "291:\tlearn: 0.6794628\ttotal: 38.5s\tremaining: 1m 33s\n",
      "292:\tlearn: 0.6794487\ttotal: 38.6s\tremaining: 1m 33s\n",
      "293:\tlearn: 0.6794381\ttotal: 38.8s\tremaining: 1m 33s\n",
      "294:\tlearn: 0.6794272\ttotal: 38.9s\tremaining: 1m 32s\n",
      "295:\tlearn: 0.6794148\ttotal: 39s\tremaining: 1m 32s\n",
      "296:\tlearn: 0.6794027\ttotal: 39.2s\tremaining: 1m 32s\n",
      "297:\tlearn: 0.6793905\ttotal: 39.3s\tremaining: 1m 32s\n",
      "298:\tlearn: 0.6793774\ttotal: 39.4s\tremaining: 1m 32s\n",
      "299:\tlearn: 0.6793655\ttotal: 39.6s\tremaining: 1m 32s\n",
      "300:\tlearn: 0.6793512\ttotal: 39.7s\tremaining: 1m 32s\n",
      "301:\tlearn: 0.6793403\ttotal: 39.9s\tremaining: 1m 32s\n",
      "302:\tlearn: 0.6793297\ttotal: 40s\tremaining: 1m 31s\n",
      "303:\tlearn: 0.6793189\ttotal: 40.1s\tremaining: 1m 31s\n",
      "304:\tlearn: 0.6793054\ttotal: 40.3s\tremaining: 1m 31s\n",
      "305:\tlearn: 0.6792923\ttotal: 40.4s\tremaining: 1m 31s\n",
      "306:\tlearn: 0.6792814\ttotal: 40.5s\tremaining: 1m 31s\n",
      "307:\tlearn: 0.6792672\ttotal: 40.7s\tremaining: 1m 31s\n",
      "308:\tlearn: 0.6792568\ttotal: 40.8s\tremaining: 1m 31s\n",
      "309:\tlearn: 0.6792437\ttotal: 41s\tremaining: 1m 31s\n",
      "310:\tlearn: 0.6792323\ttotal: 41.1s\tremaining: 1m 31s\n",
      "311:\tlearn: 0.6792198\ttotal: 41.2s\tremaining: 1m 30s\n",
      "312:\tlearn: 0.6792063\ttotal: 41.4s\tremaining: 1m 30s\n",
      "313:\tlearn: 0.6791929\ttotal: 41.5s\tremaining: 1m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314:\tlearn: 0.6791826\ttotal: 41.7s\tremaining: 1m 30s\n",
      "315:\tlearn: 0.6791706\ttotal: 41.8s\tremaining: 1m 30s\n",
      "316:\tlearn: 0.6791583\ttotal: 42s\tremaining: 1m 30s\n",
      "317:\tlearn: 0.6791466\ttotal: 42.1s\tremaining: 1m 30s\n",
      "318:\tlearn: 0.6791347\ttotal: 42.2s\tremaining: 1m 30s\n",
      "319:\tlearn: 0.6791229\ttotal: 42.3s\tremaining: 1m 29s\n",
      "320:\tlearn: 0.6791161\ttotal: 42.5s\tremaining: 1m 29s\n",
      "321:\tlearn: 0.6791027\ttotal: 42.6s\tremaining: 1m 29s\n",
      "322:\tlearn: 0.6790928\ttotal: 42.8s\tremaining: 1m 29s\n",
      "323:\tlearn: 0.6790860\ttotal: 42.9s\tremaining: 1m 29s\n",
      "324:\tlearn: 0.6790774\ttotal: 43s\tremaining: 1m 29s\n",
      "325:\tlearn: 0.6790640\ttotal: 43.2s\tremaining: 1m 29s\n",
      "326:\tlearn: 0.6790517\ttotal: 43.3s\tremaining: 1m 29s\n",
      "327:\tlearn: 0.6790407\ttotal: 43.4s\tremaining: 1m 28s\n",
      "328:\tlearn: 0.6790291\ttotal: 43.5s\tremaining: 1m 28s\n",
      "329:\tlearn: 0.6790213\ttotal: 43.7s\tremaining: 1m 28s\n",
      "330:\tlearn: 0.6790113\ttotal: 43.8s\tremaining: 1m 28s\n",
      "331:\tlearn: 0.6790012\ttotal: 43.9s\tremaining: 1m 28s\n",
      "332:\tlearn: 0.6789949\ttotal: 44.1s\tremaining: 1m 28s\n",
      "333:\tlearn: 0.6789837\ttotal: 44.2s\tremaining: 1m 28s\n",
      "334:\tlearn: 0.6789721\ttotal: 44.3s\tremaining: 1m 28s\n",
      "335:\tlearn: 0.6789633\ttotal: 44.5s\tremaining: 1m 27s\n",
      "336:\tlearn: 0.6789508\ttotal: 44.6s\tremaining: 1m 27s\n",
      "337:\tlearn: 0.6789386\ttotal: 44.7s\tremaining: 1m 27s\n",
      "338:\tlearn: 0.6789312\ttotal: 44.9s\tremaining: 1m 27s\n",
      "339:\tlearn: 0.6789206\ttotal: 45s\tremaining: 1m 27s\n",
      "340:\tlearn: 0.6789074\ttotal: 45.1s\tremaining: 1m 27s\n",
      "341:\tlearn: 0.6788978\ttotal: 45.2s\tremaining: 1m 27s\n",
      "342:\tlearn: 0.6788895\ttotal: 45.4s\tremaining: 1m 26s\n",
      "343:\tlearn: 0.6788761\ttotal: 45.5s\tremaining: 1m 26s\n",
      "344:\tlearn: 0.6788668\ttotal: 45.6s\tremaining: 1m 26s\n",
      "345:\tlearn: 0.6788543\ttotal: 45.8s\tremaining: 1m 26s\n",
      "346:\tlearn: 0.6788459\ttotal: 45.9s\tremaining: 1m 26s\n",
      "347:\tlearn: 0.6788364\ttotal: 46s\tremaining: 1m 26s\n",
      "348:\tlearn: 0.6788253\ttotal: 46.2s\tremaining: 1m 26s\n",
      "349:\tlearn: 0.6788141\ttotal: 46.3s\tremaining: 1m 25s\n",
      "350:\tlearn: 0.6788026\ttotal: 46.4s\tremaining: 1m 25s\n",
      "351:\tlearn: 0.6787899\ttotal: 46.6s\tremaining: 1m 25s\n",
      "352:\tlearn: 0.6787806\ttotal: 46.7s\tremaining: 1m 25s\n",
      "353:\tlearn: 0.6787680\ttotal: 46.9s\tremaining: 1m 25s\n",
      "354:\tlearn: 0.6787548\ttotal: 47s\tremaining: 1m 25s\n",
      "355:\tlearn: 0.6787414\ttotal: 47.1s\tremaining: 1m 25s\n",
      "356:\tlearn: 0.6787282\ttotal: 47.3s\tremaining: 1m 25s\n",
      "357:\tlearn: 0.6787163\ttotal: 47.4s\tremaining: 1m 25s\n",
      "358:\tlearn: 0.6787089\ttotal: 47.6s\tremaining: 1m 24s\n",
      "359:\tlearn: 0.6786991\ttotal: 47.7s\tremaining: 1m 24s\n",
      "360:\tlearn: 0.6786923\ttotal: 47.8s\tremaining: 1m 24s\n",
      "361:\tlearn: 0.6786775\ttotal: 48s\tremaining: 1m 24s\n",
      "362:\tlearn: 0.6786670\ttotal: 48.1s\tremaining: 1m 24s\n",
      "363:\tlearn: 0.6786567\ttotal: 48.2s\tremaining: 1m 24s\n",
      "364:\tlearn: 0.6786490\ttotal: 48.4s\tremaining: 1m 24s\n",
      "365:\tlearn: 0.6786385\ttotal: 48.5s\tremaining: 1m 24s\n",
      "366:\tlearn: 0.6786259\ttotal: 48.7s\tremaining: 1m 23s\n",
      "367:\tlearn: 0.6786171\ttotal: 48.8s\tremaining: 1m 23s\n",
      "368:\tlearn: 0.6786059\ttotal: 48.9s\tremaining: 1m 23s\n",
      "369:\tlearn: 0.6785947\ttotal: 49.1s\tremaining: 1m 23s\n",
      "370:\tlearn: 0.6785799\ttotal: 49.2s\tremaining: 1m 23s\n",
      "371:\tlearn: 0.6785681\ttotal: 49.4s\tremaining: 1m 23s\n",
      "372:\tlearn: 0.6785564\ttotal: 49.5s\tremaining: 1m 23s\n",
      "373:\tlearn: 0.6785458\ttotal: 49.7s\tremaining: 1m 23s\n",
      "374:\tlearn: 0.6785358\ttotal: 49.8s\tremaining: 1m 22s\n",
      "375:\tlearn: 0.6785256\ttotal: 49.9s\tremaining: 1m 22s\n",
      "376:\tlearn: 0.6785157\ttotal: 50.1s\tremaining: 1m 22s\n",
      "377:\tlearn: 0.6785027\ttotal: 50.2s\tremaining: 1m 22s\n",
      "378:\tlearn: 0.6784912\ttotal: 50.3s\tremaining: 1m 22s\n",
      "379:\tlearn: 0.6784811\ttotal: 50.5s\tremaining: 1m 22s\n",
      "380:\tlearn: 0.6784706\ttotal: 50.6s\tremaining: 1m 22s\n",
      "381:\tlearn: 0.6784636\ttotal: 50.8s\tremaining: 1m 22s\n",
      "382:\tlearn: 0.6784518\ttotal: 50.9s\tremaining: 1m 22s\n",
      "383:\tlearn: 0.6784440\ttotal: 51.1s\tremaining: 1m 21s\n",
      "384:\tlearn: 0.6784332\ttotal: 51.2s\tremaining: 1m 21s\n",
      "385:\tlearn: 0.6784202\ttotal: 51.3s\tremaining: 1m 21s\n",
      "386:\tlearn: 0.6784072\ttotal: 51.5s\tremaining: 1m 21s\n",
      "387:\tlearn: 0.6783974\ttotal: 51.7s\tremaining: 1m 21s\n",
      "388:\tlearn: 0.6783873\ttotal: 51.8s\tremaining: 1m 21s\n",
      "389:\tlearn: 0.6783784\ttotal: 51.9s\tremaining: 1m 21s\n",
      "390:\tlearn: 0.6783647\ttotal: 52.1s\tremaining: 1m 21s\n",
      "391:\tlearn: 0.6783564\ttotal: 52.2s\tremaining: 1m 20s\n",
      "392:\tlearn: 0.6783483\ttotal: 52.4s\tremaining: 1m 20s\n",
      "393:\tlearn: 0.6783383\ttotal: 52.5s\tremaining: 1m 20s\n",
      "394:\tlearn: 0.6783282\ttotal: 52.6s\tremaining: 1m 20s\n",
      "395:\tlearn: 0.6783199\ttotal: 52.8s\tremaining: 1m 20s\n",
      "396:\tlearn: 0.6783096\ttotal: 52.9s\tremaining: 1m 20s\n",
      "397:\tlearn: 0.6783028\ttotal: 53s\tremaining: 1m 20s\n",
      "398:\tlearn: 0.6782897\ttotal: 53.2s\tremaining: 1m 20s\n",
      "399:\tlearn: 0.6782773\ttotal: 53.3s\tremaining: 1m 19s\n",
      "400:\tlearn: 0.6782659\ttotal: 53.5s\tremaining: 1m 19s\n",
      "401:\tlearn: 0.6782530\ttotal: 53.6s\tremaining: 1m 19s\n",
      "402:\tlearn: 0.6782410\ttotal: 53.8s\tremaining: 1m 19s\n",
      "403:\tlearn: 0.6782311\ttotal: 53.9s\tremaining: 1m 19s\n",
      "404:\tlearn: 0.6782198\ttotal: 54s\tremaining: 1m 19s\n",
      "405:\tlearn: 0.6782083\ttotal: 54.2s\tremaining: 1m 19s\n",
      "406:\tlearn: 0.6781971\ttotal: 54.3s\tremaining: 1m 19s\n",
      "407:\tlearn: 0.6781837\ttotal: 54.4s\tremaining: 1m 19s\n",
      "408:\tlearn: 0.6781735\ttotal: 54.6s\tremaining: 1m 18s\n",
      "409:\tlearn: 0.6781636\ttotal: 54.7s\tremaining: 1m 18s\n",
      "410:\tlearn: 0.6781516\ttotal: 54.9s\tremaining: 1m 18s\n",
      "411:\tlearn: 0.6781434\ttotal: 55s\tremaining: 1m 18s\n",
      "412:\tlearn: 0.6781351\ttotal: 55.1s\tremaining: 1m 18s\n",
      "413:\tlearn: 0.6781251\ttotal: 55.3s\tremaining: 1m 18s\n",
      "414:\tlearn: 0.6781124\ttotal: 55.4s\tremaining: 1m 18s\n",
      "415:\tlearn: 0.6781032\ttotal: 55.5s\tremaining: 1m 17s\n",
      "416:\tlearn: 0.6780930\ttotal: 55.7s\tremaining: 1m 17s\n",
      "417:\tlearn: 0.6780830\ttotal: 55.8s\tremaining: 1m 17s\n",
      "418:\tlearn: 0.6780735\ttotal: 55.9s\tremaining: 1m 17s\n",
      "419:\tlearn: 0.6780649\ttotal: 56.1s\tremaining: 1m 17s\n",
      "420:\tlearn: 0.6780505\ttotal: 56.2s\tremaining: 1m 17s\n",
      "421:\tlearn: 0.6780377\ttotal: 56.3s\tremaining: 1m 17s\n",
      "422:\tlearn: 0.6780267\ttotal: 56.5s\tremaining: 1m 17s\n",
      "423:\tlearn: 0.6780201\ttotal: 56.6s\tremaining: 1m 16s\n",
      "424:\tlearn: 0.6780103\ttotal: 56.8s\tremaining: 1m 16s\n",
      "425:\tlearn: 0.6780002\ttotal: 56.9s\tremaining: 1m 16s\n",
      "426:\tlearn: 0.6779905\ttotal: 57s\tremaining: 1m 16s\n",
      "427:\tlearn: 0.6779813\ttotal: 57.2s\tremaining: 1m 16s\n",
      "428:\tlearn: 0.6779740\ttotal: 57.3s\tremaining: 1m 16s\n",
      "429:\tlearn: 0.6779625\ttotal: 57.4s\tremaining: 1m 16s\n",
      "430:\tlearn: 0.6779471\ttotal: 57.6s\tremaining: 1m 16s\n",
      "431:\tlearn: 0.6779358\ttotal: 57.7s\tremaining: 1m 15s\n",
      "432:\tlearn: 0.6779258\ttotal: 57.9s\tremaining: 1m 15s\n",
      "433:\tlearn: 0.6779121\ttotal: 58s\tremaining: 1m 15s\n",
      "434:\tlearn: 0.6779046\ttotal: 58.1s\tremaining: 1m 15s\n",
      "435:\tlearn: 0.6778956\ttotal: 58.3s\tremaining: 1m 15s\n",
      "436:\tlearn: 0.6778855\ttotal: 58.4s\tremaining: 1m 15s\n",
      "437:\tlearn: 0.6778740\ttotal: 58.6s\tremaining: 1m 15s\n",
      "438:\tlearn: 0.6778622\ttotal: 58.7s\tremaining: 1m 15s\n",
      "439:\tlearn: 0.6778544\ttotal: 58.8s\tremaining: 1m 14s\n",
      "440:\tlearn: 0.6778448\ttotal: 59s\tremaining: 1m 14s\n",
      "441:\tlearn: 0.6778371\ttotal: 59.1s\tremaining: 1m 14s\n",
      "442:\tlearn: 0.6778270\ttotal: 59.2s\tremaining: 1m 14s\n",
      "443:\tlearn: 0.6778161\ttotal: 59.4s\tremaining: 1m 14s\n",
      "444:\tlearn: 0.6778057\ttotal: 59.5s\tremaining: 1m 14s\n",
      "445:\tlearn: 0.6777967\ttotal: 59.7s\tremaining: 1m 14s\n",
      "446:\tlearn: 0.6777893\ttotal: 59.8s\tremaining: 1m 13s\n",
      "447:\tlearn: 0.6777776\ttotal: 59.9s\tremaining: 1m 13s\n",
      "448:\tlearn: 0.6777693\ttotal: 1m\tremaining: 1m 13s\n",
      "449:\tlearn: 0.6777589\ttotal: 1m\tremaining: 1m 13s\n",
      "450:\tlearn: 0.6777470\ttotal: 1m\tremaining: 1m 13s\n",
      "451:\tlearn: 0.6777356\ttotal: 1m\tremaining: 1m 13s\n",
      "452:\tlearn: 0.6777260\ttotal: 1m\tremaining: 1m 13s\n",
      "453:\tlearn: 0.6777172\ttotal: 1m\tremaining: 1m 13s\n",
      "454:\tlearn: 0.6777081\ttotal: 1m\tremaining: 1m 12s\n",
      "455:\tlearn: 0.6776972\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "456:\tlearn: 0.6776892\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "457:\tlearn: 0.6776758\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "458:\tlearn: 0.6776663\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "459:\tlearn: 0.6776564\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "460:\tlearn: 0.6776432\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "461:\tlearn: 0.6776316\ttotal: 1m 1s\tremaining: 1m 12s\n",
      "462:\tlearn: 0.6776201\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "463:\tlearn: 0.6776111\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "464:\tlearn: 0.6776024\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "465:\tlearn: 0.6775911\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "466:\tlearn: 0.6775794\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "467:\tlearn: 0.6775676\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "468:\tlearn: 0.6775550\ttotal: 1m 2s\tremaining: 1m 11s\n",
      "469:\tlearn: 0.6775447\ttotal: 1m 2s\tremaining: 1m 11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470:\tlearn: 0.6775328\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "471:\tlearn: 0.6775249\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "472:\tlearn: 0.6775141\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "473:\tlearn: 0.6775069\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "474:\tlearn: 0.6774985\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "475:\tlearn: 0.6774937\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "476:\tlearn: 0.6774863\ttotal: 1m 3s\tremaining: 1m 10s\n",
      "477:\tlearn: 0.6774793\ttotal: 1m 4s\tremaining: 1m 10s\n",
      "478:\tlearn: 0.6774706\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "479:\tlearn: 0.6774634\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "480:\tlearn: 0.6774508\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "481:\tlearn: 0.6774397\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "482:\tlearn: 0.6774322\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "483:\tlearn: 0.6774205\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "484:\tlearn: 0.6774103\ttotal: 1m 4s\tremaining: 1m 9s\n",
      "485:\tlearn: 0.6774037\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "486:\tlearn: 0.6773910\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "487:\tlearn: 0.6773788\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "488:\tlearn: 0.6773680\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "489:\tlearn: 0.6773590\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "490:\tlearn: 0.6773483\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "491:\tlearn: 0.6773395\ttotal: 1m 5s\tremaining: 1m 8s\n",
      "492:\tlearn: 0.6773293\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "493:\tlearn: 0.6773178\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "494:\tlearn: 0.6773074\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "495:\tlearn: 0.6772951\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "496:\tlearn: 0.6772859\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "497:\tlearn: 0.6772767\ttotal: 1m 6s\tremaining: 1m 7s\n",
      "498:\tlearn: 0.6772622\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "499:\tlearn: 0.6772524\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "500:\tlearn: 0.6772415\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "501:\tlearn: 0.6772322\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "502:\tlearn: 0.6772215\ttotal: 1m 7s\tremaining: 1m 6s\n",
      "503:\tlearn: 0.6772135\ttotal: 1m 7s\tremaining: 1m 6s\n",
      "504:\tlearn: 0.6772040\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "505:\tlearn: 0.6771950\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "506:\tlearn: 0.6771875\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "507:\tlearn: 0.6771760\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "508:\tlearn: 0.6771685\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "509:\tlearn: 0.6771593\ttotal: 1m 8s\tremaining: 1m 6s\n",
      "510:\tlearn: 0.6771487\ttotal: 1m 8s\tremaining: 1m 5s\n",
      "511:\tlearn: 0.6771377\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "512:\tlearn: 0.6771295\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "513:\tlearn: 0.6771228\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "514:\tlearn: 0.6771119\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "515:\tlearn: 0.6771012\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "516:\tlearn: 0.6770896\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "517:\tlearn: 0.6770826\ttotal: 1m 9s\tremaining: 1m 5s\n",
      "518:\tlearn: 0.6770723\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "519:\tlearn: 0.6770633\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "520:\tlearn: 0.6770540\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "521:\tlearn: 0.6770428\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "522:\tlearn: 0.6770307\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "523:\tlearn: 0.6770215\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "524:\tlearn: 0.6770150\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "525:\tlearn: 0.6770057\ttotal: 1m 10s\tremaining: 1m 3s\n",
      "526:\tlearn: 0.6769935\ttotal: 1m 11s\tremaining: 1m 3s\n",
      "527:\tlearn: 0.6769863\ttotal: 1m 11s\tremaining: 1m 3s\n",
      "528:\tlearn: 0.6769752\ttotal: 1m 11s\tremaining: 1m 3s\n",
      "529:\tlearn: 0.6769650\ttotal: 1m 11s\tremaining: 1m 3s\n",
      "530:\tlearn: 0.6769541\ttotal: 1m 11s\tremaining: 1m 3s\n",
      "531:\tlearn: 0.6769445\ttotal: 1m 12s\tremaining: 1m 3s\n",
      "532:\tlearn: 0.6769344\ttotal: 1m 12s\tremaining: 1m 3s\n",
      "533:\tlearn: 0.6769244\ttotal: 1m 12s\tremaining: 1m 3s\n",
      "534:\tlearn: 0.6769162\ttotal: 1m 12s\tremaining: 1m 3s\n",
      "535:\tlearn: 0.6769071\ttotal: 1m 12s\tremaining: 1m 2s\n",
      "536:\tlearn: 0.6769012\ttotal: 1m 12s\tremaining: 1m 2s\n",
      "537:\tlearn: 0.6768896\ttotal: 1m 12s\tremaining: 1m 2s\n",
      "538:\tlearn: 0.6768822\ttotal: 1m 13s\tremaining: 1m 2s\n",
      "539:\tlearn: 0.6768728\ttotal: 1m 13s\tremaining: 1m 2s\n",
      "540:\tlearn: 0.6768636\ttotal: 1m 13s\tremaining: 1m 2s\n",
      "541:\tlearn: 0.6768547\ttotal: 1m 13s\tremaining: 1m 2s\n",
      "542:\tlearn: 0.6768470\ttotal: 1m 13s\tremaining: 1m 1s\n",
      "543:\tlearn: 0.6768364\ttotal: 1m 13s\tremaining: 1m 1s\n",
      "544:\tlearn: 0.6768249\ttotal: 1m 13s\tremaining: 1m 1s\n",
      "545:\tlearn: 0.6768172\ttotal: 1m 14s\tremaining: 1m 1s\n",
      "546:\tlearn: 0.6768103\ttotal: 1m 14s\tremaining: 1m 1s\n",
      "547:\tlearn: 0.6767991\ttotal: 1m 14s\tremaining: 1m 1s\n",
      "548:\tlearn: 0.6767875\ttotal: 1m 14s\tremaining: 1m 1s\n",
      "549:\tlearn: 0.6767770\ttotal: 1m 14s\tremaining: 1m 1s\n",
      "550:\tlearn: 0.6767686\ttotal: 1m 14s\tremaining: 1m\n",
      "551:\tlearn: 0.6767597\ttotal: 1m 14s\tremaining: 1m\n",
      "552:\tlearn: 0.6767508\ttotal: 1m 15s\tremaining: 1m\n",
      "553:\tlearn: 0.6767441\ttotal: 1m 15s\tremaining: 1m\n",
      "554:\tlearn: 0.6767363\ttotal: 1m 15s\tremaining: 1m\n",
      "555:\tlearn: 0.6767288\ttotal: 1m 15s\tremaining: 1m\n",
      "556:\tlearn: 0.6767203\ttotal: 1m 15s\tremaining: 1m\n",
      "557:\tlearn: 0.6767057\ttotal: 1m 15s\tremaining: 1m\n",
      "558:\tlearn: 0.6766970\ttotal: 1m 15s\tremaining: 59.9s\n",
      "559:\tlearn: 0.6766877\ttotal: 1m 16s\tremaining: 59.8s\n",
      "560:\tlearn: 0.6766788\ttotal: 1m 16s\tremaining: 59.6s\n",
      "561:\tlearn: 0.6766725\ttotal: 1m 16s\tremaining: 59.5s\n",
      "562:\tlearn: 0.6766611\ttotal: 1m 16s\tremaining: 59.4s\n",
      "563:\tlearn: 0.6766515\ttotal: 1m 16s\tremaining: 59.2s\n",
      "564:\tlearn: 0.6766422\ttotal: 1m 16s\tremaining: 59.1s\n",
      "565:\tlearn: 0.6766335\ttotal: 1m 16s\tremaining: 59s\n",
      "566:\tlearn: 0.6766221\ttotal: 1m 17s\tremaining: 58.8s\n",
      "567:\tlearn: 0.6766145\ttotal: 1m 17s\tremaining: 58.7s\n",
      "568:\tlearn: 0.6766045\ttotal: 1m 17s\tremaining: 58.6s\n",
      "569:\tlearn: 0.6765927\ttotal: 1m 17s\tremaining: 58.4s\n",
      "570:\tlearn: 0.6765823\ttotal: 1m 17s\tremaining: 58.3s\n",
      "571:\tlearn: 0.6765726\ttotal: 1m 17s\tremaining: 58.2s\n",
      "572:\tlearn: 0.6765637\ttotal: 1m 17s\tremaining: 58.1s\n",
      "573:\tlearn: 0.6765554\ttotal: 1m 18s\tremaining: 58s\n",
      "574:\tlearn: 0.6765452\ttotal: 1m 18s\tremaining: 57.8s\n",
      "575:\tlearn: 0.6765350\ttotal: 1m 18s\tremaining: 57.7s\n",
      "576:\tlearn: 0.6765252\ttotal: 1m 18s\tremaining: 57.6s\n",
      "577:\tlearn: 0.6765149\ttotal: 1m 18s\tremaining: 57.5s\n",
      "578:\tlearn: 0.6765039\ttotal: 1m 18s\tremaining: 57.4s\n",
      "579:\tlearn: 0.6764926\ttotal: 1m 19s\tremaining: 57.2s\n",
      "580:\tlearn: 0.6764808\ttotal: 1m 19s\tremaining: 57.1s\n",
      "581:\tlearn: 0.6764701\ttotal: 1m 19s\tremaining: 57s\n",
      "582:\tlearn: 0.6764588\ttotal: 1m 19s\tremaining: 56.9s\n",
      "583:\tlearn: 0.6764494\ttotal: 1m 19s\tremaining: 56.8s\n",
      "584:\tlearn: 0.6764383\ttotal: 1m 19s\tremaining: 56.6s\n",
      "585:\tlearn: 0.6764282\ttotal: 1m 19s\tremaining: 56.5s\n",
      "586:\tlearn: 0.6764192\ttotal: 1m 20s\tremaining: 56.4s\n",
      "587:\tlearn: 0.6764115\ttotal: 1m 20s\tremaining: 56.3s\n",
      "588:\tlearn: 0.6764026\ttotal: 1m 20s\tremaining: 56.1s\n",
      "589:\tlearn: 0.6763943\ttotal: 1m 20s\tremaining: 56s\n",
      "590:\tlearn: 0.6763865\ttotal: 1m 20s\tremaining: 55.8s\n",
      "591:\tlearn: 0.6763777\ttotal: 1m 20s\tremaining: 55.7s\n",
      "592:\tlearn: 0.6763720\ttotal: 1m 20s\tremaining: 55.6s\n",
      "593:\tlearn: 0.6763656\ttotal: 1m 21s\tremaining: 55.5s\n",
      "594:\tlearn: 0.6763561\ttotal: 1m 21s\tremaining: 55.3s\n",
      "595:\tlearn: 0.6763449\ttotal: 1m 21s\tremaining: 55.2s\n",
      "596:\tlearn: 0.6763359\ttotal: 1m 21s\tremaining: 55.1s\n",
      "597:\tlearn: 0.6763257\ttotal: 1m 21s\tremaining: 54.9s\n",
      "598:\tlearn: 0.6763184\ttotal: 1m 21s\tremaining: 54.8s\n",
      "599:\tlearn: 0.6763108\ttotal: 1m 22s\tremaining: 54.7s\n",
      "600:\tlearn: 0.6763025\ttotal: 1m 22s\tremaining: 54.5s\n",
      "601:\tlearn: 0.6762944\ttotal: 1m 22s\tremaining: 54.4s\n",
      "602:\tlearn: 0.6762866\ttotal: 1m 22s\tremaining: 54.3s\n",
      "603:\tlearn: 0.6762759\ttotal: 1m 22s\tremaining: 54.1s\n",
      "604:\tlearn: 0.6762660\ttotal: 1m 22s\tremaining: 54s\n",
      "605:\tlearn: 0.6762579\ttotal: 1m 22s\tremaining: 53.9s\n",
      "606:\tlearn: 0.6762499\ttotal: 1m 23s\tremaining: 53.7s\n",
      "607:\tlearn: 0.6762399\ttotal: 1m 23s\tremaining: 53.6s\n",
      "608:\tlearn: 0.6762306\ttotal: 1m 23s\tremaining: 53.5s\n",
      "609:\tlearn: 0.6762236\ttotal: 1m 23s\tremaining: 53.3s\n",
      "610:\tlearn: 0.6762141\ttotal: 1m 23s\tremaining: 53.2s\n",
      "611:\tlearn: 0.6762038\ttotal: 1m 23s\tremaining: 53.1s\n",
      "612:\tlearn: 0.6761950\ttotal: 1m 23s\tremaining: 53s\n",
      "613:\tlearn: 0.6761850\ttotal: 1m 24s\tremaining: 52.9s\n",
      "614:\tlearn: 0.6761751\ttotal: 1m 24s\tremaining: 52.7s\n",
      "615:\tlearn: 0.6761657\ttotal: 1m 24s\tremaining: 52.6s\n",
      "616:\tlearn: 0.6761565\ttotal: 1m 24s\tremaining: 52.5s\n",
      "617:\tlearn: 0.6761463\ttotal: 1m 24s\tremaining: 52.4s\n",
      "618:\tlearn: 0.6761350\ttotal: 1m 24s\tremaining: 52.3s\n",
      "619:\tlearn: 0.6761280\ttotal: 1m 25s\tremaining: 52.2s\n",
      "620:\tlearn: 0.6761186\ttotal: 1m 25s\tremaining: 52.1s\n",
      "621:\tlearn: 0.6761086\ttotal: 1m 25s\tremaining: 52s\n",
      "622:\tlearn: 0.6761002\ttotal: 1m 25s\tremaining: 51.9s\n",
      "623:\tlearn: 0.6760904\ttotal: 1m 25s\tremaining: 51.8s\n",
      "624:\tlearn: 0.6760824\ttotal: 1m 26s\tremaining: 51.7s\n",
      "625:\tlearn: 0.6760776\ttotal: 1m 26s\tremaining: 51.6s\n",
      "626:\tlearn: 0.6760671\ttotal: 1m 26s\tremaining: 51.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627:\tlearn: 0.6760598\ttotal: 1m 26s\tremaining: 51.4s\n",
      "628:\tlearn: 0.6760533\ttotal: 1m 27s\tremaining: 51.3s\n",
      "629:\tlearn: 0.6760414\ttotal: 1m 27s\tremaining: 51.3s\n",
      "630:\tlearn: 0.6760336\ttotal: 1m 27s\tremaining: 51.2s\n",
      "631:\tlearn: 0.6760263\ttotal: 1m 27s\tremaining: 51.1s\n",
      "632:\tlearn: 0.6760149\ttotal: 1m 27s\tremaining: 50.9s\n",
      "633:\tlearn: 0.6760047\ttotal: 1m 28s\tremaining: 50.8s\n",
      "634:\tlearn: 0.6759949\ttotal: 1m 28s\tremaining: 50.7s\n",
      "635:\tlearn: 0.6759855\ttotal: 1m 28s\tremaining: 50.6s\n",
      "636:\tlearn: 0.6759766\ttotal: 1m 28s\tremaining: 50.5s\n",
      "637:\tlearn: 0.6759670\ttotal: 1m 28s\tremaining: 50.4s\n",
      "638:\tlearn: 0.6759560\ttotal: 1m 29s\tremaining: 50.3s\n",
      "639:\tlearn: 0.6759459\ttotal: 1m 29s\tremaining: 50.2s\n",
      "640:\tlearn: 0.6759373\ttotal: 1m 29s\tremaining: 50.1s\n",
      "641:\tlearn: 0.6759301\ttotal: 1m 29s\tremaining: 50s\n",
      "642:\tlearn: 0.6759221\ttotal: 1m 29s\tremaining: 49.9s\n",
      "643:\tlearn: 0.6759102\ttotal: 1m 30s\tremaining: 49.8s\n",
      "644:\tlearn: 0.6759048\ttotal: 1m 30s\tremaining: 49.7s\n",
      "645:\tlearn: 0.6758948\ttotal: 1m 30s\tremaining: 49.6s\n",
      "646:\tlearn: 0.6758859\ttotal: 1m 30s\tremaining: 49.5s\n",
      "647:\tlearn: 0.6758742\ttotal: 1m 30s\tremaining: 49.4s\n",
      "648:\tlearn: 0.6758617\ttotal: 1m 31s\tremaining: 49.2s\n",
      "649:\tlearn: 0.6758503\ttotal: 1m 31s\tremaining: 49.1s\n",
      "650:\tlearn: 0.6758403\ttotal: 1m 31s\tremaining: 49s\n",
      "651:\tlearn: 0.6758339\ttotal: 1m 31s\tremaining: 48.9s\n",
      "652:\tlearn: 0.6758266\ttotal: 1m 31s\tremaining: 48.8s\n",
      "653:\tlearn: 0.6758179\ttotal: 1m 32s\tremaining: 48.7s\n",
      "654:\tlearn: 0.6758088\ttotal: 1m 32s\tremaining: 48.6s\n",
      "655:\tlearn: 0.6758004\ttotal: 1m 32s\tremaining: 48.5s\n",
      "656:\tlearn: 0.6757896\ttotal: 1m 32s\tremaining: 48.4s\n",
      "657:\tlearn: 0.6757792\ttotal: 1m 32s\tremaining: 48.2s\n",
      "658:\tlearn: 0.6757709\ttotal: 1m 32s\tremaining: 48.1s\n",
      "659:\tlearn: 0.6757637\ttotal: 1m 33s\tremaining: 48s\n",
      "660:\tlearn: 0.6757585\ttotal: 1m 33s\tremaining: 47.9s\n",
      "661:\tlearn: 0.6757508\ttotal: 1m 33s\tremaining: 47.8s\n",
      "662:\tlearn: 0.6757422\ttotal: 1m 33s\tremaining: 47.7s\n",
      "663:\tlearn: 0.6757351\ttotal: 1m 33s\tremaining: 47.5s\n",
      "664:\tlearn: 0.6757261\ttotal: 1m 34s\tremaining: 47.4s\n",
      "665:\tlearn: 0.6757166\ttotal: 1m 34s\tremaining: 47.3s\n",
      "666:\tlearn: 0.6757060\ttotal: 1m 34s\tremaining: 47.2s\n",
      "667:\tlearn: 0.6756954\ttotal: 1m 34s\tremaining: 47.1s\n",
      "668:\tlearn: 0.6756867\ttotal: 1m 34s\tremaining: 46.9s\n",
      "669:\tlearn: 0.6756796\ttotal: 1m 35s\tremaining: 46.8s\n",
      "670:\tlearn: 0.6756704\ttotal: 1m 35s\tremaining: 46.7s\n",
      "671:\tlearn: 0.6756604\ttotal: 1m 35s\tremaining: 46.6s\n",
      "672:\tlearn: 0.6756536\ttotal: 1m 35s\tremaining: 46.5s\n",
      "673:\tlearn: 0.6756422\ttotal: 1m 35s\tremaining: 46.4s\n",
      "674:\tlearn: 0.6756336\ttotal: 1m 36s\tremaining: 46.3s\n",
      "675:\tlearn: 0.6756246\ttotal: 1m 36s\tremaining: 46.1s\n",
      "676:\tlearn: 0.6756161\ttotal: 1m 36s\tremaining: 46s\n",
      "677:\tlearn: 0.6756076\ttotal: 1m 36s\tremaining: 45.9s\n",
      "678:\tlearn: 0.6755995\ttotal: 1m 36s\tremaining: 45.8s\n",
      "679:\tlearn: 0.6755873\ttotal: 1m 37s\tremaining: 45.7s\n",
      "680:\tlearn: 0.6755794\ttotal: 1m 37s\tremaining: 45.6s\n",
      "681:\tlearn: 0.6755693\ttotal: 1m 37s\tremaining: 45.5s\n",
      "682:\tlearn: 0.6755589\ttotal: 1m 37s\tremaining: 45.3s\n",
      "683:\tlearn: 0.6755511\ttotal: 1m 37s\tremaining: 45.2s\n",
      "684:\tlearn: 0.6755426\ttotal: 1m 38s\tremaining: 45.1s\n",
      "685:\tlearn: 0.6755310\ttotal: 1m 38s\tremaining: 45s\n",
      "686:\tlearn: 0.6755214\ttotal: 1m 38s\tremaining: 44.8s\n",
      "687:\tlearn: 0.6755148\ttotal: 1m 38s\tremaining: 44.7s\n",
      "688:\tlearn: 0.6755055\ttotal: 1m 38s\tremaining: 44.6s\n",
      "689:\tlearn: 0.6754935\ttotal: 1m 38s\tremaining: 44.5s\n",
      "690:\tlearn: 0.6754832\ttotal: 1m 39s\tremaining: 44.3s\n",
      "691:\tlearn: 0.6754758\ttotal: 1m 39s\tremaining: 44.2s\n",
      "692:\tlearn: 0.6754647\ttotal: 1m 39s\tremaining: 44.1s\n",
      "693:\tlearn: 0.6754552\ttotal: 1m 39s\tremaining: 44s\n",
      "694:\tlearn: 0.6754443\ttotal: 1m 39s\tremaining: 43.8s\n",
      "695:\tlearn: 0.6754353\ttotal: 1m 40s\tremaining: 43.7s\n",
      "696:\tlearn: 0.6754249\ttotal: 1m 40s\tremaining: 43.6s\n",
      "697:\tlearn: 0.6754159\ttotal: 1m 40s\tremaining: 43.5s\n",
      "698:\tlearn: 0.6754095\ttotal: 1m 40s\tremaining: 43.3s\n",
      "699:\tlearn: 0.6753985\ttotal: 1m 40s\tremaining: 43.2s\n",
      "700:\tlearn: 0.6753869\ttotal: 1m 40s\tremaining: 43.1s\n",
      "701:\tlearn: 0.6753795\ttotal: 1m 41s\tremaining: 42.9s\n",
      "702:\tlearn: 0.6753707\ttotal: 1m 41s\tremaining: 42.8s\n",
      "703:\tlearn: 0.6753629\ttotal: 1m 41s\tremaining: 42.7s\n",
      "704:\tlearn: 0.6753562\ttotal: 1m 41s\tremaining: 42.6s\n",
      "705:\tlearn: 0.6753467\ttotal: 1m 41s\tremaining: 42.4s\n",
      "706:\tlearn: 0.6753347\ttotal: 1m 42s\tremaining: 42.3s\n",
      "707:\tlearn: 0.6753242\ttotal: 1m 42s\tremaining: 42.2s\n",
      "708:\tlearn: 0.6753140\ttotal: 1m 42s\tremaining: 42s\n",
      "709:\tlearn: 0.6753075\ttotal: 1m 42s\tremaining: 41.9s\n",
      "710:\tlearn: 0.6753009\ttotal: 1m 42s\tremaining: 41.8s\n",
      "711:\tlearn: 0.6752912\ttotal: 1m 42s\tremaining: 41.7s\n",
      "712:\tlearn: 0.6752829\ttotal: 1m 43s\tremaining: 41.5s\n",
      "713:\tlearn: 0.6752714\ttotal: 1m 43s\tremaining: 41.4s\n",
      "714:\tlearn: 0.6752606\ttotal: 1m 43s\tremaining: 41.3s\n",
      "715:\tlearn: 0.6752542\ttotal: 1m 43s\tremaining: 41.1s\n",
      "716:\tlearn: 0.6752459\ttotal: 1m 43s\tremaining: 41s\n",
      "717:\tlearn: 0.6752374\ttotal: 1m 44s\tremaining: 40.9s\n",
      "718:\tlearn: 0.6752286\ttotal: 1m 44s\tremaining: 40.8s\n",
      "719:\tlearn: 0.6752196\ttotal: 1m 44s\tremaining: 40.6s\n",
      "720:\tlearn: 0.6752095\ttotal: 1m 44s\tremaining: 40.5s\n",
      "721:\tlearn: 0.6751999\ttotal: 1m 44s\tremaining: 40.4s\n",
      "722:\tlearn: 0.6751942\ttotal: 1m 45s\tremaining: 40.2s\n",
      "723:\tlearn: 0.6751869\ttotal: 1m 45s\tremaining: 40.1s\n",
      "724:\tlearn: 0.6751780\ttotal: 1m 45s\tremaining: 40s\n",
      "725:\tlearn: 0.6751690\ttotal: 1m 45s\tremaining: 39.8s\n",
      "726:\tlearn: 0.6751604\ttotal: 1m 45s\tremaining: 39.7s\n",
      "727:\tlearn: 0.6751536\ttotal: 1m 45s\tremaining: 39.6s\n",
      "728:\tlearn: 0.6751439\ttotal: 1m 46s\tremaining: 39.4s\n",
      "729:\tlearn: 0.6751352\ttotal: 1m 46s\tremaining: 39.3s\n",
      "730:\tlearn: 0.6751259\ttotal: 1m 46s\tremaining: 39.2s\n",
      "731:\tlearn: 0.6751151\ttotal: 1m 46s\tremaining: 39s\n",
      "732:\tlearn: 0.6751072\ttotal: 1m 46s\tremaining: 38.9s\n",
      "733:\tlearn: 0.6750995\ttotal: 1m 47s\tremaining: 38.8s\n",
      "734:\tlearn: 0.6750907\ttotal: 1m 47s\tremaining: 38.7s\n",
      "735:\tlearn: 0.6750824\ttotal: 1m 47s\tremaining: 38.5s\n",
      "736:\tlearn: 0.6750754\ttotal: 1m 47s\tremaining: 38.4s\n",
      "737:\tlearn: 0.6750652\ttotal: 1m 47s\tremaining: 38.3s\n",
      "738:\tlearn: 0.6750560\ttotal: 1m 47s\tremaining: 38.1s\n",
      "739:\tlearn: 0.6750450\ttotal: 1m 48s\tremaining: 38s\n",
      "740:\tlearn: 0.6750334\ttotal: 1m 48s\tremaining: 37.9s\n",
      "741:\tlearn: 0.6750260\ttotal: 1m 48s\tremaining: 37.7s\n",
      "742:\tlearn: 0.6750183\ttotal: 1m 48s\tremaining: 37.6s\n",
      "743:\tlearn: 0.6750106\ttotal: 1m 48s\tremaining: 37.4s\n",
      "744:\tlearn: 0.6750047\ttotal: 1m 48s\tremaining: 37.3s\n",
      "745:\tlearn: 0.6749970\ttotal: 1m 49s\tremaining: 37.2s\n",
      "746:\tlearn: 0.6749861\ttotal: 1m 49s\tremaining: 37s\n",
      "747:\tlearn: 0.6749777\ttotal: 1m 49s\tremaining: 36.9s\n",
      "748:\tlearn: 0.6749670\ttotal: 1m 49s\tremaining: 36.8s\n",
      "749:\tlearn: 0.6749587\ttotal: 1m 49s\tremaining: 36.6s\n",
      "750:\tlearn: 0.6749497\ttotal: 1m 50s\tremaining: 36.5s\n",
      "751:\tlearn: 0.6749413\ttotal: 1m 50s\tremaining: 36.4s\n",
      "752:\tlearn: 0.6749335\ttotal: 1m 50s\tremaining: 36.2s\n",
      "753:\tlearn: 0.6749237\ttotal: 1m 50s\tremaining: 36.1s\n",
      "754:\tlearn: 0.6749149\ttotal: 1m 50s\tremaining: 35.9s\n",
      "755:\tlearn: 0.6749048\ttotal: 1m 50s\tremaining: 35.8s\n",
      "756:\tlearn: 0.6748946\ttotal: 1m 51s\tremaining: 35.7s\n",
      "757:\tlearn: 0.6748840\ttotal: 1m 51s\tremaining: 35.5s\n",
      "758:\tlearn: 0.6748754\ttotal: 1m 51s\tremaining: 35.4s\n",
      "759:\tlearn: 0.6748684\ttotal: 1m 51s\tremaining: 35.3s\n",
      "760:\tlearn: 0.6748593\ttotal: 1m 51s\tremaining: 35.1s\n",
      "761:\tlearn: 0.6748509\ttotal: 1m 51s\tremaining: 35s\n",
      "762:\tlearn: 0.6748450\ttotal: 1m 52s\tremaining: 34.8s\n",
      "763:\tlearn: 0.6748361\ttotal: 1m 52s\tremaining: 34.7s\n",
      "764:\tlearn: 0.6748244\ttotal: 1m 52s\tremaining: 34.6s\n",
      "765:\tlearn: 0.6748140\ttotal: 1m 52s\tremaining: 34.4s\n",
      "766:\tlearn: 0.6748082\ttotal: 1m 52s\tremaining: 34.3s\n",
      "767:\tlearn: 0.6748008\ttotal: 1m 53s\tremaining: 34.2s\n",
      "768:\tlearn: 0.6747897\ttotal: 1m 53s\tremaining: 34s\n",
      "769:\tlearn: 0.6747801\ttotal: 1m 53s\tremaining: 33.9s\n",
      "770:\tlearn: 0.6747694\ttotal: 1m 53s\tremaining: 33.7s\n",
      "771:\tlearn: 0.6747616\ttotal: 1m 53s\tremaining: 33.6s\n",
      "772:\tlearn: 0.6747555\ttotal: 1m 53s\tremaining: 33.5s\n",
      "773:\tlearn: 0.6747468\ttotal: 1m 54s\tremaining: 33.3s\n",
      "774:\tlearn: 0.6747408\ttotal: 1m 54s\tremaining: 33.2s\n",
      "775:\tlearn: 0.6747338\ttotal: 1m 54s\tremaining: 33s\n",
      "776:\tlearn: 0.6747233\ttotal: 1m 54s\tremaining: 32.9s\n",
      "777:\tlearn: 0.6747126\ttotal: 1m 54s\tremaining: 32.8s\n",
      "778:\tlearn: 0.6747069\ttotal: 1m 54s\tremaining: 32.6s\n",
      "779:\tlearn: 0.6746974\ttotal: 1m 55s\tremaining: 32.5s\n",
      "780:\tlearn: 0.6746906\ttotal: 1m 55s\tremaining: 32.3s\n",
      "781:\tlearn: 0.6746813\ttotal: 1m 55s\tremaining: 32.2s\n",
      "782:\tlearn: 0.6746741\ttotal: 1m 55s\tremaining: 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783:\tlearn: 0.6746668\ttotal: 1m 55s\tremaining: 31.9s\n",
      "784:\tlearn: 0.6746590\ttotal: 1m 55s\tremaining: 31.8s\n",
      "785:\tlearn: 0.6746535\ttotal: 1m 56s\tremaining: 31.6s\n",
      "786:\tlearn: 0.6746469\ttotal: 1m 56s\tremaining: 31.5s\n",
      "787:\tlearn: 0.6746354\ttotal: 1m 56s\tremaining: 31.3s\n",
      "788:\tlearn: 0.6746267\ttotal: 1m 56s\tremaining: 31.2s\n",
      "789:\tlearn: 0.6746188\ttotal: 1m 56s\tremaining: 31s\n",
      "790:\tlearn: 0.6746108\ttotal: 1m 56s\tremaining: 30.9s\n",
      "791:\tlearn: 0.6746002\ttotal: 1m 57s\tremaining: 30.7s\n",
      "792:\tlearn: 0.6745898\ttotal: 1m 57s\tremaining: 30.6s\n",
      "793:\tlearn: 0.6745825\ttotal: 1m 57s\tremaining: 30.5s\n",
      "794:\tlearn: 0.6745756\ttotal: 1m 57s\tremaining: 30.3s\n",
      "795:\tlearn: 0.6745680\ttotal: 1m 57s\tremaining: 30.2s\n",
      "796:\tlearn: 0.6745586\ttotal: 1m 57s\tremaining: 30s\n",
      "797:\tlearn: 0.6745510\ttotal: 1m 58s\tremaining: 29.9s\n",
      "798:\tlearn: 0.6745409\ttotal: 1m 58s\tremaining: 29.7s\n",
      "799:\tlearn: 0.6745335\ttotal: 1m 58s\tremaining: 29.6s\n",
      "800:\tlearn: 0.6745260\ttotal: 1m 58s\tremaining: 29.5s\n",
      "801:\tlearn: 0.6745183\ttotal: 1m 58s\tremaining: 29.3s\n",
      "802:\tlearn: 0.6745063\ttotal: 1m 58s\tremaining: 29.2s\n",
      "803:\tlearn: 0.6744987\ttotal: 1m 59s\tremaining: 29s\n",
      "804:\tlearn: 0.6744882\ttotal: 1m 59s\tremaining: 28.9s\n",
      "805:\tlearn: 0.6744799\ttotal: 1m 59s\tremaining: 28.7s\n",
      "806:\tlearn: 0.6744714\ttotal: 1m 59s\tremaining: 28.6s\n",
      "807:\tlearn: 0.6744621\ttotal: 1m 59s\tremaining: 28.5s\n",
      "808:\tlearn: 0.6744546\ttotal: 1m 59s\tremaining: 28.3s\n",
      "809:\tlearn: 0.6744473\ttotal: 2m\tremaining: 28.2s\n",
      "810:\tlearn: 0.6744401\ttotal: 2m\tremaining: 28s\n",
      "811:\tlearn: 0.6744318\ttotal: 2m\tremaining: 27.9s\n",
      "812:\tlearn: 0.6744229\ttotal: 2m\tremaining: 27.7s\n",
      "813:\tlearn: 0.6744138\ttotal: 2m\tremaining: 27.6s\n",
      "814:\tlearn: 0.6744048\ttotal: 2m\tremaining: 27.4s\n",
      "815:\tlearn: 0.6743979\ttotal: 2m 1s\tremaining: 27.3s\n",
      "816:\tlearn: 0.6743893\ttotal: 2m 1s\tremaining: 27.1s\n",
      "817:\tlearn: 0.6743807\ttotal: 2m 1s\tremaining: 27s\n",
      "818:\tlearn: 0.6743705\ttotal: 2m 1s\tremaining: 26.9s\n",
      "819:\tlearn: 0.6743610\ttotal: 2m 1s\tremaining: 26.7s\n",
      "820:\tlearn: 0.6743528\ttotal: 2m 1s\tremaining: 26.6s\n",
      "821:\tlearn: 0.6743455\ttotal: 2m 2s\tremaining: 26.4s\n",
      "822:\tlearn: 0.6743369\ttotal: 2m 2s\tremaining: 26.3s\n",
      "823:\tlearn: 0.6743285\ttotal: 2m 2s\tremaining: 26.1s\n",
      "824:\tlearn: 0.6743176\ttotal: 2m 2s\tremaining: 26s\n",
      "825:\tlearn: 0.6743103\ttotal: 2m 2s\tremaining: 25.8s\n",
      "826:\tlearn: 0.6742997\ttotal: 2m 2s\tremaining: 25.7s\n",
      "827:\tlearn: 0.6742883\ttotal: 2m 3s\tremaining: 25.6s\n",
      "828:\tlearn: 0.6742791\ttotal: 2m 3s\tremaining: 25.4s\n",
      "829:\tlearn: 0.6742734\ttotal: 2m 3s\tremaining: 25.3s\n",
      "830:\tlearn: 0.6742655\ttotal: 2m 3s\tremaining: 25.1s\n",
      "831:\tlearn: 0.6742549\ttotal: 2m 3s\tremaining: 25s\n",
      "832:\tlearn: 0.6742485\ttotal: 2m 3s\tremaining: 24.8s\n",
      "833:\tlearn: 0.6742382\ttotal: 2m 4s\tremaining: 24.7s\n",
      "834:\tlearn: 0.6742288\ttotal: 2m 4s\tremaining: 24.5s\n",
      "835:\tlearn: 0.6742196\ttotal: 2m 4s\tremaining: 24.4s\n",
      "836:\tlearn: 0.6742111\ttotal: 2m 4s\tremaining: 24.2s\n",
      "837:\tlearn: 0.6742016\ttotal: 2m 4s\tremaining: 24.1s\n",
      "838:\tlearn: 0.6741946\ttotal: 2m 4s\tremaining: 23.9s\n",
      "839:\tlearn: 0.6741859\ttotal: 2m 4s\tremaining: 23.8s\n",
      "840:\tlearn: 0.6741803\ttotal: 2m 5s\tremaining: 23.7s\n",
      "841:\tlearn: 0.6741733\ttotal: 2m 5s\tremaining: 23.5s\n",
      "842:\tlearn: 0.6741650\ttotal: 2m 5s\tremaining: 23.4s\n",
      "843:\tlearn: 0.6741574\ttotal: 2m 5s\tremaining: 23.2s\n",
      "844:\tlearn: 0.6741490\ttotal: 2m 5s\tremaining: 23.1s\n",
      "845:\tlearn: 0.6741395\ttotal: 2m 5s\tremaining: 22.9s\n",
      "846:\tlearn: 0.6741317\ttotal: 2m 6s\tremaining: 22.8s\n",
      "847:\tlearn: 0.6741211\ttotal: 2m 6s\tremaining: 22.6s\n",
      "848:\tlearn: 0.6741135\ttotal: 2m 6s\tremaining: 22.5s\n",
      "849:\tlearn: 0.6741040\ttotal: 2m 6s\tremaining: 22.3s\n",
      "850:\tlearn: 0.6740932\ttotal: 2m 6s\tremaining: 22.2s\n",
      "851:\tlearn: 0.6740847\ttotal: 2m 6s\tremaining: 22s\n",
      "852:\tlearn: 0.6740787\ttotal: 2m 6s\tremaining: 21.9s\n",
      "853:\tlearn: 0.6740706\ttotal: 2m 7s\tremaining: 21.7s\n",
      "854:\tlearn: 0.6740613\ttotal: 2m 7s\tremaining: 21.6s\n",
      "855:\tlearn: 0.6740527\ttotal: 2m 7s\tremaining: 21.4s\n",
      "856:\tlearn: 0.6740421\ttotal: 2m 7s\tremaining: 21.3s\n",
      "857:\tlearn: 0.6740327\ttotal: 2m 7s\tremaining: 21.1s\n",
      "858:\tlearn: 0.6740257\ttotal: 2m 7s\tremaining: 21s\n",
      "859:\tlearn: 0.6740158\ttotal: 2m 8s\tremaining: 20.8s\n",
      "860:\tlearn: 0.6740089\ttotal: 2m 8s\tremaining: 20.7s\n",
      "861:\tlearn: 0.6740005\ttotal: 2m 8s\tremaining: 20.5s\n",
      "862:\tlearn: 0.6739886\ttotal: 2m 8s\tremaining: 20.4s\n",
      "863:\tlearn: 0.6739802\ttotal: 2m 8s\tremaining: 20.2s\n",
      "864:\tlearn: 0.6739690\ttotal: 2m 8s\tremaining: 20.1s\n",
      "865:\tlearn: 0.6739614\ttotal: 2m 8s\tremaining: 19.9s\n",
      "866:\tlearn: 0.6739569\ttotal: 2m 9s\tremaining: 19.8s\n",
      "867:\tlearn: 0.6739463\ttotal: 2m 9s\tremaining: 19.6s\n",
      "868:\tlearn: 0.6739375\ttotal: 2m 9s\tremaining: 19.5s\n",
      "869:\tlearn: 0.6739283\ttotal: 2m 9s\tremaining: 19.4s\n",
      "870:\tlearn: 0.6739199\ttotal: 2m 9s\tremaining: 19.2s\n",
      "871:\tlearn: 0.6739098\ttotal: 2m 9s\tremaining: 19.1s\n",
      "872:\tlearn: 0.6739033\ttotal: 2m 10s\tremaining: 18.9s\n",
      "873:\tlearn: 0.6738941\ttotal: 2m 10s\tremaining: 18.8s\n",
      "874:\tlearn: 0.6738845\ttotal: 2m 10s\tremaining: 18.6s\n",
      "875:\tlearn: 0.6738754\ttotal: 2m 10s\tremaining: 18.5s\n",
      "876:\tlearn: 0.6738674\ttotal: 2m 10s\tremaining: 18.3s\n",
      "877:\tlearn: 0.6738581\ttotal: 2m 10s\tremaining: 18.2s\n",
      "878:\tlearn: 0.6738493\ttotal: 2m 10s\tremaining: 18s\n",
      "879:\tlearn: 0.6738418\ttotal: 2m 11s\tremaining: 17.9s\n",
      "880:\tlearn: 0.6738359\ttotal: 2m 11s\tremaining: 17.7s\n",
      "881:\tlearn: 0.6738293\ttotal: 2m 11s\tremaining: 17.6s\n",
      "882:\tlearn: 0.6738195\ttotal: 2m 11s\tremaining: 17.4s\n",
      "883:\tlearn: 0.6738117\ttotal: 2m 11s\tremaining: 17.3s\n",
      "884:\tlearn: 0.6738007\ttotal: 2m 11s\tremaining: 17.1s\n",
      "885:\tlearn: 0.6737925\ttotal: 2m 11s\tremaining: 17s\n",
      "886:\tlearn: 0.6737844\ttotal: 2m 12s\tremaining: 16.8s\n",
      "887:\tlearn: 0.6737745\ttotal: 2m 12s\tremaining: 16.7s\n",
      "888:\tlearn: 0.6737677\ttotal: 2m 12s\tremaining: 16.5s\n",
      "889:\tlearn: 0.6737573\ttotal: 2m 12s\tremaining: 16.4s\n",
      "890:\tlearn: 0.6737498\ttotal: 2m 12s\tremaining: 16.2s\n",
      "891:\tlearn: 0.6737417\ttotal: 2m 12s\tremaining: 16.1s\n",
      "892:\tlearn: 0.6737333\ttotal: 2m 12s\tremaining: 15.9s\n",
      "893:\tlearn: 0.6737236\ttotal: 2m 13s\tremaining: 15.8s\n",
      "894:\tlearn: 0.6737151\ttotal: 2m 13s\tremaining: 15.6s\n",
      "895:\tlearn: 0.6737044\ttotal: 2m 13s\tremaining: 15.5s\n",
      "896:\tlearn: 0.6736966\ttotal: 2m 13s\tremaining: 15.3s\n",
      "897:\tlearn: 0.6736861\ttotal: 2m 13s\tremaining: 15.2s\n",
      "898:\tlearn: 0.6736787\ttotal: 2m 13s\tremaining: 15s\n",
      "899:\tlearn: 0.6736685\ttotal: 2m 13s\tremaining: 14.9s\n",
      "900:\tlearn: 0.6736601\ttotal: 2m 14s\tremaining: 14.7s\n",
      "901:\tlearn: 0.6736512\ttotal: 2m 14s\tremaining: 14.6s\n",
      "902:\tlearn: 0.6736434\ttotal: 2m 14s\tremaining: 14.4s\n",
      "903:\tlearn: 0.6736327\ttotal: 2m 14s\tremaining: 14.3s\n",
      "904:\tlearn: 0.6736249\ttotal: 2m 14s\tremaining: 14.1s\n",
      "905:\tlearn: 0.6736156\ttotal: 2m 14s\tremaining: 14s\n",
      "906:\tlearn: 0.6736067\ttotal: 2m 14s\tremaining: 13.8s\n",
      "907:\tlearn: 0.6735985\ttotal: 2m 15s\tremaining: 13.7s\n",
      "908:\tlearn: 0.6735906\ttotal: 2m 15s\tremaining: 13.5s\n",
      "909:\tlearn: 0.6735824\ttotal: 2m 15s\tremaining: 13.4s\n",
      "910:\tlearn: 0.6735741\ttotal: 2m 15s\tremaining: 13.2s\n",
      "911:\tlearn: 0.6735636\ttotal: 2m 15s\tremaining: 13.1s\n",
      "912:\tlearn: 0.6735510\ttotal: 2m 15s\tremaining: 13s\n",
      "913:\tlearn: 0.6735379\ttotal: 2m 16s\tremaining: 12.8s\n",
      "914:\tlearn: 0.6735318\ttotal: 2m 16s\tremaining: 12.7s\n",
      "915:\tlearn: 0.6735229\ttotal: 2m 16s\tremaining: 12.5s\n",
      "916:\tlearn: 0.6735117\ttotal: 2m 16s\tremaining: 12.4s\n",
      "917:\tlearn: 0.6735051\ttotal: 2m 16s\tremaining: 12.2s\n",
      "918:\tlearn: 0.6734968\ttotal: 2m 17s\tremaining: 12.1s\n",
      "919:\tlearn: 0.6734873\ttotal: 2m 17s\tremaining: 11.9s\n",
      "920:\tlearn: 0.6734778\ttotal: 2m 17s\tremaining: 11.8s\n",
      "921:\tlearn: 0.6734717\ttotal: 2m 17s\tremaining: 11.7s\n",
      "922:\tlearn: 0.6734644\ttotal: 2m 17s\tremaining: 11.5s\n",
      "923:\tlearn: 0.6734571\ttotal: 2m 18s\tremaining: 11.4s\n",
      "924:\tlearn: 0.6734484\ttotal: 2m 18s\tremaining: 11.2s\n",
      "925:\tlearn: 0.6734417\ttotal: 2m 18s\tremaining: 11.1s\n",
      "926:\tlearn: 0.6734324\ttotal: 2m 18s\tremaining: 10.9s\n",
      "927:\tlearn: 0.6734238\ttotal: 2m 18s\tremaining: 10.8s\n",
      "928:\tlearn: 0.6734171\ttotal: 2m 18s\tremaining: 10.6s\n",
      "929:\tlearn: 0.6734104\ttotal: 2m 19s\tremaining: 10.5s\n",
      "930:\tlearn: 0.6734055\ttotal: 2m 19s\tremaining: 10.3s\n",
      "931:\tlearn: 0.6733966\ttotal: 2m 19s\tremaining: 10.2s\n",
      "932:\tlearn: 0.6733903\ttotal: 2m 19s\tremaining: 10s\n",
      "933:\tlearn: 0.6733836\ttotal: 2m 19s\tremaining: 9.88s\n",
      "934:\tlearn: 0.6733737\ttotal: 2m 19s\tremaining: 9.73s\n",
      "935:\tlearn: 0.6733662\ttotal: 2m 20s\tremaining: 9.58s\n",
      "936:\tlearn: 0.6733576\ttotal: 2m 20s\tremaining: 9.43s\n",
      "937:\tlearn: 0.6733482\ttotal: 2m 20s\tremaining: 9.28s\n",
      "938:\tlearn: 0.6733397\ttotal: 2m 20s\tremaining: 9.13s\n",
      "939:\tlearn: 0.6733302\ttotal: 2m 20s\tremaining: 8.98s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940:\tlearn: 0.6733206\ttotal: 2m 20s\tremaining: 8.83s\n",
      "941:\tlearn: 0.6733146\ttotal: 2m 20s\tremaining: 8.68s\n",
      "942:\tlearn: 0.6733081\ttotal: 2m 21s\tremaining: 8.53s\n",
      "943:\tlearn: 0.6732983\ttotal: 2m 21s\tremaining: 8.38s\n",
      "944:\tlearn: 0.6732915\ttotal: 2m 21s\tremaining: 8.23s\n",
      "945:\tlearn: 0.6732831\ttotal: 2m 21s\tremaining: 8.08s\n",
      "946:\tlearn: 0.6732738\ttotal: 2m 21s\tremaining: 7.93s\n",
      "947:\tlearn: 0.6732642\ttotal: 2m 21s\tremaining: 7.78s\n",
      "948:\tlearn: 0.6732579\ttotal: 2m 21s\tremaining: 7.63s\n",
      "949:\tlearn: 0.6732496\ttotal: 2m 22s\tremaining: 7.48s\n",
      "950:\tlearn: 0.6732401\ttotal: 2m 22s\tremaining: 7.33s\n",
      "951:\tlearn: 0.6732323\ttotal: 2m 22s\tremaining: 7.18s\n",
      "952:\tlearn: 0.6732265\ttotal: 2m 22s\tremaining: 7.03s\n",
      "953:\tlearn: 0.6732180\ttotal: 2m 22s\tremaining: 6.88s\n",
      "954:\tlearn: 0.6732108\ttotal: 2m 22s\tremaining: 6.73s\n",
      "955:\tlearn: 0.6732030\ttotal: 2m 22s\tremaining: 6.58s\n",
      "956:\tlearn: 0.6731950\ttotal: 2m 23s\tremaining: 6.43s\n",
      "957:\tlearn: 0.6731856\ttotal: 2m 23s\tremaining: 6.28s\n",
      "958:\tlearn: 0.6731779\ttotal: 2m 23s\tremaining: 6.13s\n",
      "959:\tlearn: 0.6731700\ttotal: 2m 23s\tremaining: 5.98s\n",
      "960:\tlearn: 0.6731621\ttotal: 2m 23s\tremaining: 5.83s\n",
      "961:\tlearn: 0.6731554\ttotal: 2m 23s\tremaining: 5.68s\n",
      "962:\tlearn: 0.6731471\ttotal: 2m 23s\tremaining: 5.53s\n",
      "963:\tlearn: 0.6731369\ttotal: 2m 24s\tremaining: 5.38s\n",
      "964:\tlearn: 0.6731293\ttotal: 2m 24s\tremaining: 5.23s\n",
      "965:\tlearn: 0.6731205\ttotal: 2m 24s\tremaining: 5.08s\n",
      "966:\tlearn: 0.6731108\ttotal: 2m 24s\tremaining: 4.93s\n",
      "967:\tlearn: 0.6731019\ttotal: 2m 24s\tremaining: 4.78s\n",
      "968:\tlearn: 0.6730930\ttotal: 2m 24s\tremaining: 4.63s\n",
      "969:\tlearn: 0.6730831\ttotal: 2m 24s\tremaining: 4.48s\n",
      "970:\tlearn: 0.6730735\ttotal: 2m 25s\tremaining: 4.33s\n",
      "971:\tlearn: 0.6730626\ttotal: 2m 25s\tremaining: 4.18s\n",
      "972:\tlearn: 0.6730530\ttotal: 2m 25s\tremaining: 4.03s\n",
      "973:\tlearn: 0.6730470\ttotal: 2m 25s\tremaining: 3.88s\n",
      "974:\tlearn: 0.6730380\ttotal: 2m 25s\tremaining: 3.73s\n",
      "975:\tlearn: 0.6730287\ttotal: 2m 25s\tremaining: 3.59s\n",
      "976:\tlearn: 0.6730236\ttotal: 2m 25s\tremaining: 3.44s\n",
      "977:\tlearn: 0.6730145\ttotal: 2m 26s\tremaining: 3.29s\n",
      "978:\tlearn: 0.6730051\ttotal: 2m 26s\tremaining: 3.14s\n",
      "979:\tlearn: 0.6729986\ttotal: 2m 26s\tremaining: 2.99s\n",
      "980:\tlearn: 0.6729903\ttotal: 2m 26s\tremaining: 2.84s\n",
      "981:\tlearn: 0.6729817\ttotal: 2m 26s\tremaining: 2.69s\n",
      "982:\tlearn: 0.6729739\ttotal: 2m 26s\tremaining: 2.54s\n",
      "983:\tlearn: 0.6729663\ttotal: 2m 26s\tremaining: 2.39s\n",
      "984:\tlearn: 0.6729585\ttotal: 2m 27s\tremaining: 2.24s\n",
      "985:\tlearn: 0.6729469\ttotal: 2m 27s\tremaining: 2.09s\n",
      "986:\tlearn: 0.6729385\ttotal: 2m 27s\tremaining: 1.94s\n",
      "987:\tlearn: 0.6729288\ttotal: 2m 27s\tremaining: 1.79s\n",
      "988:\tlearn: 0.6729215\ttotal: 2m 27s\tremaining: 1.64s\n",
      "989:\tlearn: 0.6729141\ttotal: 2m 27s\tremaining: 1.49s\n",
      "990:\tlearn: 0.6729074\ttotal: 2m 27s\tremaining: 1.34s\n",
      "991:\tlearn: 0.6728989\ttotal: 2m 28s\tremaining: 1.19s\n",
      "992:\tlearn: 0.6728900\ttotal: 2m 28s\tremaining: 1.04s\n",
      "993:\tlearn: 0.6728820\ttotal: 2m 28s\tremaining: 895ms\n",
      "994:\tlearn: 0.6728716\ttotal: 2m 28s\tremaining: 746ms\n",
      "995:\tlearn: 0.6728637\ttotal: 2m 28s\tremaining: 597ms\n",
      "996:\tlearn: 0.6728547\ttotal: 2m 28s\tremaining: 448ms\n",
      "997:\tlearn: 0.6728476\ttotal: 2m 28s\tremaining: 298ms\n",
      "998:\tlearn: 0.6728376\ttotal: 2m 29s\tremaining: 149ms\n",
      "999:\tlearn: 0.6728299\ttotal: 2m 29s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5694444444444444"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier()                                    # 학습 데이터 더 많아야 됨\n",
    "cat.fit(train_x, train_y)\n",
    "accuracy_score(cat.predict(test_x), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61b7ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.save_model('models/cat_model_to_2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec1c169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578124074993283"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(cat.predict(train_x), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f83fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat2 = CatBoostClassifier()\n",
    "# min_features_to_select = 1\n",
    "# selector = RFECV(cat2, step=1, cv=5, min_features_to_select = min_features_to_select )\n",
    "# selector = selector.fit(train_x, train_y)\n",
    "\n",
    "# print(selector.support_)\n",
    "\n",
    "# print(selector.ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(\n",
    "    range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n",
    "    rfecv.grid_scores_,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95d2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc_AUC :  0.5410216718266254\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-41aa49b44662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mft_importance_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mft_importance_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "pred_train = cat.predict_proba(test_x)\n",
    "\n",
    "test_x\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true=test_y, y_score=pred_train[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('Roc_AUC : ', roc_auc)  \n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"ROC curve\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ft_importance_values = xgb.feature_importances_\n",
    "\n",
    "ft_importance_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57b7a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458923, 16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe47b24",
   "metadata": {},
   "source": [
    "> LSTM을 쓰면;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09faa204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>RSI</th>\n",
       "      <th>STOCASTIC_K</th>\n",
       "      <th>STOCASTIC_D</th>\n",
       "      <th>Bollinger</th>\n",
       "      <th>...</th>\n",
       "      <th>RSI_delta</th>\n",
       "      <th>D_delta</th>\n",
       "      <th>sto_diff</th>\n",
       "      <th>B_delta</th>\n",
       "      <th>MACD_delta</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA5_adj</th>\n",
       "      <th>MA20_adj</th>\n",
       "      <th>MA_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-12-20</th>\n",
       "      <td>6247</td>\n",
       "      <td>6323</td>\n",
       "      <td>6226</td>\n",
       "      <td>6319</td>\n",
       "      <td>3756714</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.448341</td>\n",
       "      <td>0.528217</td>\n",
       "      <td>0.417295</td>\n",
       "      <td>0.552657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030153</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>0.110922</td>\n",
       "      <td>0.043872</td>\n",
       "      <td>1.053686</td>\n",
       "      <td>6248.0</td>\n",
       "      <td>6297.45</td>\n",
       "      <td>-0.011236</td>\n",
       "      <td>-0.003410</td>\n",
       "      <td>-0.007826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-23</th>\n",
       "      <td>6299</td>\n",
       "      <td>6333</td>\n",
       "      <td>6143</td>\n",
       "      <td>6162</td>\n",
       "      <td>3876679</td>\n",
       "      <td>-0.024846</td>\n",
       "      <td>0.368376</td>\n",
       "      <td>0.173815</td>\n",
       "      <td>0.336135</td>\n",
       "      <td>0.165915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079965</td>\n",
       "      <td>-0.081160</td>\n",
       "      <td>-0.162320</td>\n",
       "      <td>-0.386742</td>\n",
       "      <td>-12.091285</td>\n",
       "      <td>6226.4</td>\n",
       "      <td>6297.90</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>-0.011603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-24</th>\n",
       "      <td>6107</td>\n",
       "      <td>6165</td>\n",
       "      <td>5989</td>\n",
       "      <td>6019</td>\n",
       "      <td>3582444</td>\n",
       "      <td>-0.023207</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>0.069284</td>\n",
       "      <td>0.247184</td>\n",
       "      <td>-0.062924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072849</td>\n",
       "      <td>-0.088950</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>-0.228839</td>\n",
       "      <td>-21.170494</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>6285.65</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>0.044301</td>\n",
       "      <td>-0.012735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-26</th>\n",
       "      <td>5991</td>\n",
       "      <td>6046</td>\n",
       "      <td>5921</td>\n",
       "      <td>6027</td>\n",
       "      <td>3787692</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.354930</td>\n",
       "      <td>0.226496</td>\n",
       "      <td>0.240288</td>\n",
       "      <td>0.023921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059402</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.013793</td>\n",
       "      <td>0.086845</td>\n",
       "      <td>-16.214307</td>\n",
       "      <td>6163.8</td>\n",
       "      <td>6275.85</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.041289</td>\n",
       "      <td>-0.018591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-27</th>\n",
       "      <td>5929</td>\n",
       "      <td>6020</td>\n",
       "      <td>5896</td>\n",
       "      <td>5916</td>\n",
       "      <td>2643288</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.173798</td>\n",
       "      <td>-0.060703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071911</td>\n",
       "      <td>-0.066491</td>\n",
       "      <td>-0.132981</td>\n",
       "      <td>-0.084624</td>\n",
       "      <td>-21.490256</td>\n",
       "      <td>6088.6</td>\n",
       "      <td>6257.80</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.057776</td>\n",
       "      <td>-0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>35471</td>\n",
       "      <td>35839</td>\n",
       "      <td>35169</td>\n",
       "      <td>35793</td>\n",
       "      <td>16393168</td>\n",
       "      <td>0.014196</td>\n",
       "      <td>0.612858</td>\n",
       "      <td>0.713564</td>\n",
       "      <td>0.736792</td>\n",
       "      <td>0.682016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>-0.011614</td>\n",
       "      <td>-0.023229</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-39.042743</td>\n",
       "      <td>35734.6</td>\n",
       "      <td>35217.90</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>-0.016067</td>\n",
       "      <td>0.014436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>35858</td>\n",
       "      <td>36635</td>\n",
       "      <td>35858</td>\n",
       "      <td>36506</td>\n",
       "      <td>10416934</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.620894</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>0.872924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.060312</td>\n",
       "      <td>0.120625</td>\n",
       "      <td>0.190907</td>\n",
       "      <td>16.766654</td>\n",
       "      <td>35859.8</td>\n",
       "      <td>35348.55</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>0.014005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>36735</td>\n",
       "      <td>37006</td>\n",
       "      <td>36543</td>\n",
       "      <td>36690</td>\n",
       "      <td>10206477</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.620027</td>\n",
       "      <td>0.837029</td>\n",
       "      <td>0.810413</td>\n",
       "      <td>0.896177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>18.101461</td>\n",
       "      <td>36031.4</td>\n",
       "      <td>35484.50</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>-0.032856</td>\n",
       "      <td>0.014906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>37031</td>\n",
       "      <td>37343</td>\n",
       "      <td>36591</td>\n",
       "      <td>37313</td>\n",
       "      <td>17032623</td>\n",
       "      <td>0.016980</td>\n",
       "      <td>0.753332</td>\n",
       "      <td>0.986201</td>\n",
       "      <td>0.869009</td>\n",
       "      <td>1.076831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133304</td>\n",
       "      <td>0.058596</td>\n",
       "      <td>0.117192</td>\n",
       "      <td>0.180654</td>\n",
       "      <td>53.960909</td>\n",
       "      <td>36318.8</td>\n",
       "      <td>35677.45</td>\n",
       "      <td>-0.026645</td>\n",
       "      <td>-0.043833</td>\n",
       "      <td>0.017188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>37151</td>\n",
       "      <td>38182</td>\n",
       "      <td>37074</td>\n",
       "      <td>38098</td>\n",
       "      <td>13205479</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>0.751931</td>\n",
       "      <td>0.972121</td>\n",
       "      <td>0.903379</td>\n",
       "      <td>1.209023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.068741</td>\n",
       "      <td>0.132192</td>\n",
       "      <td>94.364056</td>\n",
       "      <td>36880.0</td>\n",
       "      <td>35883.10</td>\n",
       "      <td>-0.031970</td>\n",
       "      <td>-0.058137</td>\n",
       "      <td>0.026167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4459 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close    Volume    Change       RSI  \\\n",
       "Date                                                                   \n",
       "2002-12-20   6247   6323   6226   6319   3756714  0.004291  0.448341   \n",
       "2002-12-23   6299   6333   6143   6162   3876679 -0.024846  0.368376   \n",
       "2002-12-24   6107   6165   5989   6019   3582444 -0.023207  0.295527   \n",
       "2002-12-26   5991   6046   5921   6027   3787692  0.001329  0.354930   \n",
       "2002-12-27   5929   6020   5896   5916   2643288 -0.018417  0.283019   \n",
       "...           ...    ...    ...    ...       ...       ...       ...   \n",
       "2020-12-23  35471  35839  35169  35793  16393168  0.014196  0.612858   \n",
       "2020-12-24  35858  36635  35858  36506  10416934  0.019920  0.620894   \n",
       "2020-12-28  36735  37006  36543  36690  10206477  0.005040  0.620027   \n",
       "2020-12-29  37031  37343  36591  37313  17032623  0.016980  0.753332   \n",
       "2020-12-30  37151  38182  37074  38098  13205479  0.021038  0.751931   \n",
       "\n",
       "            STOCASTIC_K  STOCASTIC_D  Bollinger  ...  RSI_delta   D_delta  \\\n",
       "Date                                             ...                        \n",
       "2002-12-20     0.528217     0.417295   0.552657  ...  -0.030153  0.055461   \n",
       "2002-12-23     0.173815     0.336135   0.165915  ...  -0.079965 -0.081160   \n",
       "2002-12-24     0.069284     0.247184  -0.062924  ...  -0.072849 -0.088950   \n",
       "2002-12-26     0.226496     0.240288   0.023921  ...   0.059402 -0.006896   \n",
       "2002-12-27     0.040816     0.173798  -0.060703  ...  -0.071911 -0.066491   \n",
       "...                 ...          ...        ...  ...        ...       ...   \n",
       "2020-12-23     0.713564     0.736792   0.682016  ...   0.024448 -0.011614   \n",
       "2020-12-24     0.917730     0.797105   0.872924  ...   0.008036  0.060312   \n",
       "2020-12-28     0.837029     0.810413   0.896177  ...  -0.000867  0.013308   \n",
       "2020-12-29     0.986201     0.869009   1.076831  ...   0.133304  0.058596   \n",
       "2020-12-30     0.972121     0.903379   1.209023  ...  -0.001401  0.034371   \n",
       "\n",
       "            sto_diff   B_delta  MACD_delta      MA5      MA20   MA5_adj  \\\n",
       "Date                                                                      \n",
       "2002-12-20  0.110922  0.043872    1.053686   6248.0   6297.45 -0.011236   \n",
       "2002-12-23 -0.162320 -0.386742  -12.091285   6226.4   6297.90  0.010451   \n",
       "2002-12-24 -0.177900 -0.228839  -21.170494   6209.0   6285.65  0.031567   \n",
       "2002-12-26 -0.013793  0.086845  -16.214307   6163.8   6275.85  0.022698   \n",
       "2002-12-27 -0.132981 -0.084624  -21.490256   6088.6   6257.80  0.029175   \n",
       "...              ...       ...         ...      ...       ...       ...   \n",
       "2020-12-23 -0.023229  0.127762  -39.042743  35734.6  35217.90 -0.001632   \n",
       "2020-12-24  0.120625  0.190907   16.766654  35859.8  35348.55 -0.017701   \n",
       "2020-12-28  0.026617  0.023253   18.101461  36031.4  35484.50 -0.017950   \n",
       "2020-12-29  0.117192  0.180654   53.960909  36318.8  35677.45 -0.026645   \n",
       "2020-12-30  0.068741  0.132192   94.364056  36880.0  35883.10 -0.031970   \n",
       "\n",
       "            MA20_adj   MA_diff  \n",
       "Date                            \n",
       "2002-12-20 -0.003410 -0.007826  \n",
       "2002-12-23  0.022055 -0.011603  \n",
       "2002-12-24  0.044301 -0.012735  \n",
       "2002-12-26  0.041289 -0.018591  \n",
       "2002-12-27  0.057776 -0.028600  \n",
       "...              ...       ...  \n",
       "2020-12-23 -0.016067  0.014436  \n",
       "2020-12-24 -0.031706  0.014005  \n",
       "2020-12-28 -0.032856  0.014906  \n",
       "2020-12-29 -0.043833  0.017188  \n",
       "2020-12-30 -0.058137  0.026167  \n",
       "\n",
       "[4459 rows x 24 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kospi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "afd40b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4459, 17)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6f87b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 17)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1d69424a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSI</th>\n",
       "      <th>STOCASTIC_K</th>\n",
       "      <th>STOCASTIC_D</th>\n",
       "      <th>Bollinger</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_cat</th>\n",
       "      <th>MACD_SIGNAL</th>\n",
       "      <th>RSI_delta</th>\n",
       "      <th>D_delta</th>\n",
       "      <th>sto_diff</th>\n",
       "      <th>B_delta</th>\n",
       "      <th>MACD_delta</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA5_adj</th>\n",
       "      <th>MA20_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-12-20</th>\n",
       "      <td>0.448341</td>\n",
       "      <td>0.528217</td>\n",
       "      <td>0.417295</td>\n",
       "      <td>0.552657</td>\n",
       "      <td>55.980216</td>\n",
       "      <td>0</td>\n",
       "      <td>74.979173</td>\n",
       "      <td>-0.030153</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>0.110922</td>\n",
       "      <td>0.043872</td>\n",
       "      <td>1.053686</td>\n",
       "      <td>6248.0</td>\n",
       "      <td>6297.45</td>\n",
       "      <td>-0.011236</td>\n",
       "      <td>-0.003410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-23</th>\n",
       "      <td>0.368376</td>\n",
       "      <td>0.173815</td>\n",
       "      <td>0.336135</td>\n",
       "      <td>0.165915</td>\n",
       "      <td>43.888930</td>\n",
       "      <td>0</td>\n",
       "      <td>68.761036</td>\n",
       "      <td>-0.079965</td>\n",
       "      <td>-0.081160</td>\n",
       "      <td>-0.162320</td>\n",
       "      <td>-0.386742</td>\n",
       "      <td>-12.091285</td>\n",
       "      <td>6226.4</td>\n",
       "      <td>6297.90</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.022055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-24</th>\n",
       "      <td>0.295527</td>\n",
       "      <td>0.069284</td>\n",
       "      <td>0.247184</td>\n",
       "      <td>-0.062924</td>\n",
       "      <td>22.718436</td>\n",
       "      <td>0</td>\n",
       "      <td>59.552411</td>\n",
       "      <td>-0.072849</td>\n",
       "      <td>-0.088950</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>-0.228839</td>\n",
       "      <td>-21.170494</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>6285.65</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>0.044301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-26</th>\n",
       "      <td>0.354930</td>\n",
       "      <td>0.226496</td>\n",
       "      <td>0.240288</td>\n",
       "      <td>0.023921</td>\n",
       "      <td>6.504130</td>\n",
       "      <td>0</td>\n",
       "      <td>48.942658</td>\n",
       "      <td>0.059402</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.013793</td>\n",
       "      <td>0.086845</td>\n",
       "      <td>-16.214307</td>\n",
       "      <td>6163.8</td>\n",
       "      <td>6275.85</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.041289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-27</th>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.173798</td>\n",
       "      <td>-0.060703</td>\n",
       "      <td>-14.986126</td>\n",
       "      <td>0</td>\n",
       "      <td>36.156808</td>\n",
       "      <td>-0.071911</td>\n",
       "      <td>-0.066491</td>\n",
       "      <td>-0.132981</td>\n",
       "      <td>-0.084624</td>\n",
       "      <td>-21.490256</td>\n",
       "      <td>6088.6</td>\n",
       "      <td>6257.80</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.057776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>0.612858</td>\n",
       "      <td>0.713564</td>\n",
       "      <td>0.736792</td>\n",
       "      <td>0.682016</td>\n",
       "      <td>835.537551</td>\n",
       "      <td>0</td>\n",
       "      <td>966.700231</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>-0.011614</td>\n",
       "      <td>-0.023229</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-39.042743</td>\n",
       "      <td>35734.6</td>\n",
       "      <td>35217.90</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>-0.016067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>0.620894</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>0.872924</td>\n",
       "      <td>852.304205</td>\n",
       "      <td>0</td>\n",
       "      <td>943.821026</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.060312</td>\n",
       "      <td>0.120625</td>\n",
       "      <td>0.190907</td>\n",
       "      <td>16.766654</td>\n",
       "      <td>35859.8</td>\n",
       "      <td>35348.55</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>-0.031706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>0.620027</td>\n",
       "      <td>0.837029</td>\n",
       "      <td>0.810413</td>\n",
       "      <td>0.896177</td>\n",
       "      <td>870.405666</td>\n",
       "      <td>0</td>\n",
       "      <td>929.137954</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>18.101461</td>\n",
       "      <td>36031.4</td>\n",
       "      <td>35484.50</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>-0.032856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>0.753332</td>\n",
       "      <td>0.986201</td>\n",
       "      <td>0.869009</td>\n",
       "      <td>1.076831</td>\n",
       "      <td>924.366575</td>\n",
       "      <td>0</td>\n",
       "      <td>928.183678</td>\n",
       "      <td>0.133304</td>\n",
       "      <td>0.058596</td>\n",
       "      <td>0.117192</td>\n",
       "      <td>0.180654</td>\n",
       "      <td>53.960909</td>\n",
       "      <td>36318.8</td>\n",
       "      <td>35677.45</td>\n",
       "      <td>-0.026645</td>\n",
       "      <td>-0.043833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>0.751931</td>\n",
       "      <td>0.972121</td>\n",
       "      <td>0.903379</td>\n",
       "      <td>1.209023</td>\n",
       "      <td>1018.730632</td>\n",
       "      <td>0</td>\n",
       "      <td>946.293069</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.068741</td>\n",
       "      <td>0.132192</td>\n",
       "      <td>94.364056</td>\n",
       "      <td>36880.0</td>\n",
       "      <td>35883.10</td>\n",
       "      <td>-0.031970</td>\n",
       "      <td>-0.058137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4459 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RSI  STOCASTIC_K  STOCASTIC_D  Bollinger         MACD  \\\n",
       "Date                                                                     \n",
       "2002-12-20  0.448341     0.528217     0.417295   0.552657    55.980216   \n",
       "2002-12-23  0.368376     0.173815     0.336135   0.165915    43.888930   \n",
       "2002-12-24  0.295527     0.069284     0.247184  -0.062924    22.718436   \n",
       "2002-12-26  0.354930     0.226496     0.240288   0.023921     6.504130   \n",
       "2002-12-27  0.283019     0.040816     0.173798  -0.060703   -14.986126   \n",
       "...              ...          ...          ...        ...          ...   \n",
       "2020-12-23  0.612858     0.713564     0.736792   0.682016   835.537551   \n",
       "2020-12-24  0.620894     0.917730     0.797105   0.872924   852.304205   \n",
       "2020-12-28  0.620027     0.837029     0.810413   0.896177   870.405666   \n",
       "2020-12-29  0.753332     0.986201     0.869009   1.076831   924.366575   \n",
       "2020-12-30  0.751931     0.972121     0.903379   1.209023  1018.730632   \n",
       "\n",
       "            MACD_cat  MACD_SIGNAL  RSI_delta   D_delta  sto_diff   B_delta  \\\n",
       "Date                                                                         \n",
       "2002-12-20         0    74.979173  -0.030153  0.055461  0.110922  0.043872   \n",
       "2002-12-23         0    68.761036  -0.079965 -0.081160 -0.162320 -0.386742   \n",
       "2002-12-24         0    59.552411  -0.072849 -0.088950 -0.177900 -0.228839   \n",
       "2002-12-26         0    48.942658   0.059402 -0.006896 -0.013793  0.086845   \n",
       "2002-12-27         0    36.156808  -0.071911 -0.066491 -0.132981 -0.084624   \n",
       "...              ...          ...        ...       ...       ...       ...   \n",
       "2020-12-23         0   966.700231   0.024448 -0.011614 -0.023229  0.127762   \n",
       "2020-12-24         0   943.821026   0.008036  0.060312  0.120625  0.190907   \n",
       "2020-12-28         0   929.137954  -0.000867  0.013308  0.026617  0.023253   \n",
       "2020-12-29         0   928.183678   0.133304  0.058596  0.117192  0.180654   \n",
       "2020-12-30         0   946.293069  -0.001401  0.034371  0.068741  0.132192   \n",
       "\n",
       "            MACD_delta      MA5      MA20   MA5_adj  MA20_adj  \n",
       "Date                                                           \n",
       "2002-12-20    1.053686   6248.0   6297.45 -0.011236 -0.003410  \n",
       "2002-12-23  -12.091285   6226.4   6297.90  0.010451  0.022055  \n",
       "2002-12-24  -21.170494   6209.0   6285.65  0.031567  0.044301  \n",
       "2002-12-26  -16.214307   6163.8   6275.85  0.022698  0.041289  \n",
       "2002-12-27  -21.490256   6088.6   6257.80  0.029175  0.057776  \n",
       "...                ...      ...       ...       ...       ...  \n",
       "2020-12-23  -39.042743  35734.6  35217.90 -0.001632 -0.016067  \n",
       "2020-12-24   16.766654  35859.8  35348.55 -0.017701 -0.031706  \n",
       "2020-12-28   18.101461  36031.4  35484.50 -0.017950 -0.032856  \n",
       "2020-12-29   53.960909  36318.8  35677.45 -0.026645 -0.043833  \n",
       "2020-12-30   94.364056  36880.0  35883.10 -0.031970 -0.058137  \n",
       "\n",
       "[4459 rows x 16 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1ac5be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(20, len(train_x)):\n",
    "    x_train.append(train_x[i - 20 : i])\n",
    "    y_train.append(train_y[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a1b86361",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2c00dcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4439, 20, 17)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4b33be6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4439, 20, 17)\n",
      "(4439,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9b773c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 20, 17)\n",
      "(299,)\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(20, len(test_x)):\n",
    "    x_test.append(test_x[i-20:i])\n",
    "    y_test.append(test_y[i - 1])\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_features = ['RSI', 'STOCASTIC_K', 'STOCASTIC_D', 'Bollinger', 'MACD', 'MACD_SIGNAL', 'RSI_delta', 'D_delta', 'sto_diff', 'B_delta', 'MACD_delta', 'MA5', 'MA20',\n",
    "#                    'MA5_adj', 'MA20_adj']\n",
    "# numeric_transformer = StandardScaler() # cf) RobustScaler\n",
    "\n",
    "categorical_features = ['MACD_cat']\n",
    "categorical_transformer = OneHotEncoder(categories='auto') # categories='auto' : just for ignoring warning messages\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [ ('num', numeric_transformer, numeric_features),\n",
    "        ('passthrough', categorical_transformer, categorical_features)])\n",
    "\n",
    "preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "preprocessor_pipe.fit(train_x)\n",
    "\n",
    "train_x = preprocessor_pipe.transform(train_x)\n",
    "test_x = preprocessor_pipe.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "57765c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train = enc.fit_transform(np.reshape(y_train, (len(y_train), 1)))\n",
    "y_test = enc.fit_transform(np.reshape(y_test, (len(y_test), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e8c3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "537bfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8bd2419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 5s 12ms/step - loss: 1.7035 - accuracy: 0.4812\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.7251 - accuracy: 0.4882\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.7111 - accuracy: 0.4970\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.7043 - accuracy: 0.5062\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.7000 - accuracy: 0.5112\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6974 - accuracy: 0.5123\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6957 - accuracy: 0.5208\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6945 - accuracy: 0.5163\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6938 - accuracy: 0.5217\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6933 - accuracy: 0.5202\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit(x_train, y_train, batch_size=40, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777562d",
   "metadata": {},
   "source": [
    "> 튜닝?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8dd2460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e733d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 20, 17)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fbbee147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \n",
    "    L2_NUM_FILTERS = hp.Int('input_unit',min_value=25,max_value=100,step=25)\n",
    "    L1_NUM_FILTERS = hp.Int('input_unit',min_value=25,max_value=100,step=25)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(L1_NUM_FILTERS, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "#     for i in range(hp.Int('n_layers', 0, 2)):\n",
    "#         model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=25,max_value=100,step=25),return_sequences=True))\n",
    "    model.add(LSTM(L2_NUM_FILTERS, return_sequences=False))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(),\n",
    "                loss = 'binary_crossentropy', \n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f6bd5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 1\n",
      "input_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 25, 'max_value': 100, 'step': 25, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(model_builder,\n",
    "                                objective = 'val_accuracy', # Hyper-params tuning을 위한 목적함수 설정 (metric to minimize or maximize)\n",
    "                                max_trials = 50, # 서로 다른 Hyper-params 조합으로 시도할 총 Trial 횟수 설정\n",
    "                                directory = 'datasets', # Path to the working directory\n",
    "                                project_name = 'mymodel'\n",
    "                                ) # Name to use as directory name for files saved by this Tuner\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9b1fd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "25                |?                 |input_unit\n",
      "\n",
      "Epoch 1/10\n",
      "138/139 [============================>.] - ETA: 0s - loss: 0.7072 - accuracy: 0.5177"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1323 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1266 test_step\n        y_pred = self(x, training=False)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer sequential: expected shape=(None, None, 16), found shape=(None, 20, 17)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-b70d7c7707fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         return tuner_utils.convert_to_metrics_dict(\n\u001b[0;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HyperModel.fit()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1214\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1323 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1266 test_step\n        y_pred = self(x, training=False)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer sequential: expected shape=(None, None, 16), found shape=(None, 20, 17)\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=10, validation_data = (x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
